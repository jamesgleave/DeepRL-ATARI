{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BOHhCVqpCRX4"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X4twE_dhE0uT"
      },
      "outputs": [],
      "source": [
        "import urllib.request\n",
        "urllib.request.urlretrieve('http://www.atarimania.com/roms/Roms.rar','Roms.rar')\n",
        "!pip install unrar\n",
        "!unrar x Roms.rar\n",
        "!mkdir rars\n",
        "!mv HC\\ ROMS.zip   rars\n",
        "!mv ROMS.zip  rars\n",
        "!python -m atari_py.import_roms rars"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iZm_o2kkE2zS"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# James Note: Importing deque for stack\n",
        "from collections import deque\n",
        "\n",
        "\n",
        "class Atari(object):\n",
        "    def __init__(self, game: str, resized_width: int, resized_height: int, frame_skip=4, clip_reward: bool = True):\n",
        "        \"\"\"[summary]\n",
        "\n",
        "        Args:\n",
        "            game (str): [description]\n",
        "            resized_width (int): [description]\n",
        "            resized_height (int): [description]\n",
        "            frame_skip (int, optional): [description]. Defaults to 4.\n",
        "            clip_reward (bool, optional): [description]. Defaults to True.\n",
        "        \"\"\"\n",
        "        self.env = gym.make(game)\n",
        "        self.action_space_size = self.env.action_space.n  # The number of actions an agent can perform (int)\n",
        "        self.resized_width = resized_width\n",
        "        self.resized_height = resized_height\n",
        "\n",
        "        # James Note: Added these two attrs for frame stacking\n",
        "        self._frame_stack = deque([], frame_skip)\n",
        "        self.frame_skip = frame_skip\n",
        "        self.output = None\n",
        "\n",
        "        # Also clip reward\n",
        "        self.clip_reward = clip_reward\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"[summary]\n",
        "\n",
        "        Returns:\n",
        "            [type]: [description]\n",
        "        \"\"\"\n",
        "        # Should reset the environment to the begining\n",
        "        # Returns initial state\n",
        "        # James Note: Implemented stacking from reset\n",
        "        reset_obs = self.get_preprocessed_frame(self.env.reset())\n",
        "        for _ in range(self.frame_skip):\n",
        "            self._frame_stack.append(reset_obs)\n",
        "        self.output = np.concatenate(self._frame_stack, axis=-1)\n",
        "        return self.output\n",
        "\n",
        "    def num_actions_available(self):\n",
        "        \"\"\"[summary]\n",
        "\n",
        "        Returns:\n",
        "            [type]: [description]\n",
        "        \"\"\"\n",
        "        # Return total number of actions\n",
        "        return self.env.action_space.n\n",
        "\n",
        "    def get_preprocessed_frame(self, observation):\n",
        "        \"\"\"[summary]\n",
        "\n",
        "        Args:\n",
        "            observation ([type]): [description]\n",
        "\n",
        "        Returns:\n",
        "            [type]: [description]\n",
        "        \"\"\"\n",
        "        # Convert image to grayscale\n",
        "        # Rescale image\n",
        "        # James Note: Rewrote this method to return a uint8 and expand the dims\n",
        "        img = observation[34:-16, :, :]\n",
        "        # Resize image\n",
        "        img = cv2.resize(img, (84,84))\n",
        "        # Grayscale\n",
        "        img = img.mean(-1,keepdims=True)\n",
        "        # Return as an unsigned integer to save space\n",
        "        # Normalization occurs upon model input\n",
        "        return img.astype(\"uint8\")\n",
        "\n",
        "\n",
        "    def get_action_meanings(self):\n",
        "        \"\"\"[summary]\n",
        "\n",
        "        Returns:\n",
        "            [type]: [description]\n",
        "        \"\"\"\n",
        "        # Prints meaings of all possible actions\n",
        "        return self.env.get_action_meanings()\n",
        "\n",
        "    def step(self, action: int, plot_frames: bool = False) -> tuple:\n",
        "        \"\"\"[summary]\n",
        "\n",
        "        Args:\n",
        "            action (int): [description]\n",
        "            frame_skip (int, optional): [description]. Defaults to 1.\n",
        "\n",
        "        Returns:\n",
        "            tuple: [description]\n",
        "        \"\"\"\n",
        "\n",
        "        # Note from James\n",
        "        # Frame skip was not implemented >>> Now it is (rewrote method)\n",
        "\n",
        "        total_reward = 0\n",
        "        second_to_last = None\n",
        "        last = None\n",
        "        for i in range(self.frame_skip):\n",
        "            # take a step in the env and return the next state, reward,\n",
        "            # and if the game is done as a tuple\n",
        "            observation, reward, done, info = self.env.step(action)\n",
        "\n",
        "            # Sum up the total reward\n",
        "            total_reward += reward\n",
        "\n",
        "            # Store the second to last frame\n",
        "            if i == self.frame_skip - 1:\n",
        "                last = self.get_preprocessed_frame(observation)\n",
        "            elif i == self.frame_skip - 2:\n",
        "                second_to_last = self.get_preprocessed_frame(observation)\n",
        "\n",
        "        # Get the max of the last frame\n",
        "        max_frame = np.array([second_to_last, last]).max(axis=0)\n",
        "\n",
        "        # Append the newest frame\n",
        "        self._frame_stack.append(max_frame)\n",
        "\n",
        "        # If the num of stacks is correct then we concat\n",
        "        assert len(self._frame_stack) == self.frame_skip or self.output is not None, \"Must Reset Env To Step\"\n",
        "        self.output = np.concatenate(self._frame_stack, axis=-1)\n",
        "\n",
        "        if plot_frames:\n",
        "            import matplotlib.pyplot as plt\n",
        "            f, axarr = plt.subplots(2,2)\n",
        "\n",
        "            axarr[0,0].imshow((self.output[:, :, 0] / 255.0).astype(np.float32))\n",
        "            axarr[0,0].set_title(\"Frame-1\")\n",
        "\n",
        "            axarr[0,1].imshow((self.output[:, :, 1] / 255.0).astype(np.float32))\n",
        "            axarr[0,1].set_title(\"Frame-2\")\n",
        "\n",
        "            axarr[1,0].imshow((self.output[:, :, 2] / 255.0).astype(np.float32))\n",
        "            axarr[1,0].set_title(\"Frame-3\")\n",
        "\n",
        "            axarr[1,1].imshow((self.output[:, :, 3] / 255.0).astype(np.float32))\n",
        "            axarr[1,1].set_title(\"Frame-4\")\n",
        "            plt.show()\n",
        "\n",
        "        # Clip the reward\n",
        "        if self.clip_reward:\n",
        "            total_reward = np.clip(total_reward, -1, 1)\n",
        "\n",
        "        return self.output[:, :, ::-1], total_reward, done, info\n",
        "\n",
        "    def render(self):\n",
        "        # Render the game state\n",
        "        self.env.render()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    game = Atari(\"Breakout-v4\", 84, 84)\n",
        "    game.reset()\n",
        "    print(game.step(0, plot_frames=True)[0].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install tensorflow==2.1.0\n",
        "!pip install keras-rl2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "97DlypLuE_Z4"
      },
      "outputs": [],
      "source": [
        "from __future__ import division\n",
        "import argparse\n",
        "\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import gym\n",
        "\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Flatten, Convolution2D, Permute\n",
        "from keras.optimizers import Adam\n",
        "import keras.backend as K\n",
        "\n",
        "import rl\n",
        "from rl.agents.dqn import DQNAgent\n",
        "from rl.policy import LinearAnnealedPolicy, BoltzmannQPolicy, EpsGreedyQPolicy\n",
        "from rl.memory import SequentialMemory\n",
        "from rl.core import Processor\n",
        "from rl.callbacks import FileLogger, ModelIntervalCheckpoint\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "\n",
        "\n",
        "WINDOW_LENGTH = 1\n",
        "\n",
        "\n",
        "class AtariProcessor(Processor):\n",
        "    def process_observation(self, observation):\n",
        "        return observation.astype('uint8')\n",
        "    def process_state_batch(self, batch):\n",
        "        processed_batch = batch.astype('float32') / 255.\n",
        "        return processed_batch.reshape((processed_batch.shape[0], 84, 84, 4))\n",
        "    def process_reward(self, reward):\n",
        "        return np.clip(reward, -1., 1.)\n",
        "\n",
        "# Create our env and seed our randoms\n",
        "env = Atari(\"BreakoutNoFrameskip-v4\", 84, 84, frame_skip=4)\n",
        "np.random.seed(42)\n",
        "env.env.seed(42)\n",
        "num_actions = env.action_space_size\n",
        "\n",
        "# First layer takes in the 4 grayscale cropped image\n",
        "input_lyr = keras.layers.Input((84,84,4), name=\"Input_last_4_frames\")\n",
        "# Convolutional layers \n",
        "x = keras.layers.Conv2D(32, (8,8), activation='relu', strides=4, use_bias=False, name=\"Hidden_layer_1\")(input_lyr)\n",
        "x = keras.layers.Conv2D(64, (4,4), activation='relu', strides=2, use_bias=False, name=\"Hidden_layer_2\")(x)\n",
        "x = keras.layers.Conv2D(64, (3,3), activation='relu', strides=1, use_bias=False, name=\"Hidden_layer_3\")(x)\n",
        "x = keras.layers.Conv2D(1024, (7,7), activation='relu', strides=1, use_bias=False, name=\"Hidden_layer_4\")(x)\n",
        "# Flattening for dense output\n",
        "x = keras.layers.Flatten(name=\"Final_flatten\")(x)\n",
        "x = keras.layers.Dense(num_actions, activation='linear')(x)\n",
        "model = keras.Model(inputs=input_lyr, outputs=x, name=\"ATARI_DQN\")\n",
        "\n",
        "# Set up the replay memory, processor, policy\n",
        "memory = SequentialMemory(limit=250_000, window_length=WINDOW_LENGTH)\n",
        "processor = AtariProcessor()\n",
        "policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), attr='eps', value_max=1., value_min=.1, value_test=.05, nb_steps=1000000)\n",
        "\n",
        "\n",
        "dqn = DQNAgent(model=model, \n",
        "               nb_actions=num_actions, \n",
        "               policy=policy, \n",
        "               memory=memory, \n",
        "               processor=processor, \n",
        "               nb_steps_warmup=50_000, \n",
        "               gamma=.99, \n",
        "               target_model_update=10000, \n",
        "               train_interval=4, \n",
        "               delta_clip=1.)\n",
        "dqn.compile(Adam(lr=.00025), metrics=['mae'])\n",
        "\n",
        "# Create our file names\n",
        "weights_filename = 'weights_final.h5f'\n",
        "checkpoint_weights_filename = '_weights_{step}.h5f'\n",
        "log_filename = 'dqn_log.json'\n",
        "callbacks = [ModelIntervalCheckpoint(checkpoint_weights_filename, interval=150_000)]\n",
        "callbacks += [FileLogger(log_filename, interval=5)]\n",
        "dqn.fit(env, callbacks=callbacks, nb_steps=5_000_000, log_interval=10000)\n",
        "\n",
        "# Save the final weights\n",
        "dqn.save_weights(weights_filename, overwrite=True)\n",
        "\n",
        "# Save the model as well\n",
        "model.save(\"dqn_model.h5\")\n",
        "\n",
        "# Test the dqn\n",
        "dqn.test(env, nb_episodes=10, visualize=False)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Untitled19.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
