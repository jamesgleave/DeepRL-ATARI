{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Ip_ioeat8AZ",
        "outputId": "928abad4-479a-4b05-e713-49a722892a0e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-5RkowKBusaF",
        "outputId": "b2caacc5-033f-4af6-d8be-839e87688dde"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/University/Fourth Year/CISC_474_project_part_3\n"
          ]
        }
      ],
      "source": [
        "%cd drive/MyDrive/University/Fourth\\ Year/CISC_474_project_part_3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IAG20BdPkh3y",
        "outputId": "bedf99fe-bd8b-485f-fca8-147e5a3e1c61"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: unrar in /usr/local/lib/python3.7/dist-packages (0.4)\n",
            "\n",
            "UNRAR 5.50 freeware      Copyright (c) 1993-2017 Alexander Roshal\n",
            "\n",
            "\n",
            "Extracting from Roms.rar\n",
            "\n",
            "Extracting  HC ROMS.zip                                                  \b\b\b\b 36%\b\b\b\b\b  OK \n",
            "Extracting  ROMS.zip                                                     \b\b\b\b 74%\b\b\b\b 99%\b\b\b\b\b  OK \n",
            "All OK\n",
            "copying adventure.bin from HC ROMS/BY ALPHABET (PAL)/A-G/Adventure (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/adventure.bin\n",
            "copying air_raid.bin from HC ROMS/BY ALPHABET (PAL)/A-G/Air Raid (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/air_raid.bin\n",
            "copying alien.bin from HC ROMS/BY ALPHABET (PAL)/A-G/REMAINING NTSC ORIGINALS/Alien.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/alien.bin\n",
            "copying crazy_climber.bin from HC ROMS/BY ALPHABET (PAL)/A-G/REMAINING NTSC ORIGINALS/Crazy Climber.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/crazy_climber.bin\n",
            "copying elevator_action.bin from HC ROMS/BY ALPHABET (PAL)/A-G/REMAINING NTSC ORIGINALS/Elevator Action (Prototype).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/elevator_action.bin\n",
            "copying gravitar.bin from HC ROMS/BY ALPHABET (PAL)/A-G/REMAINING NTSC ORIGINALS/Gravitar.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/gravitar.bin\n",
            "copying keystone_kapers.bin from HC ROMS/BY ALPHABET (PAL)/H-R/Keystone Kapers (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/keystone_kapers.bin\n",
            "copying king_kong.bin from HC ROMS/BY ALPHABET (PAL)/H-R/King Kong (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/king_kong.bin\n",
            "copying laser_gates.bin from HC ROMS/BY ALPHABET (PAL)/H-R/Laser Gates (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/laser_gates.bin\n",
            "copying mr_do.bin from HC ROMS/BY ALPHABET (PAL)/H-R/Mr. Do! (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/mr_do.bin\n",
            "copying pacman.bin from HC ROMS/BY ALPHABET (PAL)/H-R/Pac-Man (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/pacman.bin\n",
            "copying jamesbond.bin from HC ROMS/BY ALPHABET (PAL)/H-R/REMAINING NTSC ORIGINALS/James Bond 007.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/jamesbond.bin\n",
            "copying koolaid.bin from HC ROMS/BY ALPHABET (PAL)/H-R/REMAINING NTSC ORIGINALS/Kool-Aid Man.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/koolaid.bin\n",
            "copying krull.bin from HC ROMS/BY ALPHABET (PAL)/H-R/REMAINING NTSC ORIGINALS/Krull.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/krull.bin\n",
            "copying montezuma_revenge.bin from HC ROMS/BY ALPHABET (PAL)/H-R/REMAINING NTSC ORIGINALS/Montezuma's Revenge - Featuring Panama Joe.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/montezuma_revenge.bin\n",
            "copying star_gunner.bin from HC ROMS/BY ALPHABET (PAL)/S-Z/REMAINING NTSC ORIGINALS/Stargunner.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/star_gunner.bin\n",
            "copying time_pilot.bin from HC ROMS/BY ALPHABET (PAL)/S-Z/REMAINING NTSC ORIGINALS/Time Pilot.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/time_pilot.bin\n",
            "copying up_n_down.bin from HC ROMS/BY ALPHABET (PAL)/S-Z/REMAINING NTSC ORIGINALS/Up 'n Down.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/up_n_down.bin\n",
            "copying sir_lancelot.bin from HC ROMS/BY ALPHABET (PAL)/S-Z/Sir Lancelot (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/sir_lancelot.bin\n",
            "copying amidar.bin from HC ROMS/BY ALPHABET/A-G/Amidar.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/amidar.bin\n",
            "copying asteroids.bin from HC ROMS/BY ALPHABET/A-G/Asteroids [no copyright].bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/asteroids.bin\n",
            "copying atlantis.bin from HC ROMS/BY ALPHABET/A-G/Atlantis.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/atlantis.bin\n",
            "copying bank_heist.bin from HC ROMS/BY ALPHABET/A-G/Bank Heist.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/bank_heist.bin\n",
            "copying battle_zone.bin from HC ROMS/BY ALPHABET/A-G/Battlezone.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/battle_zone.bin\n",
            "copying beam_rider.bin from HC ROMS/BY ALPHABET/A-G/Beamrider.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/beam_rider.bin\n",
            "copying berzerk.bin from HC ROMS/BY ALPHABET/A-G/Berzerk.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/berzerk.bin\n",
            "copying bowling.bin from HC ROMS/BY ALPHABET/A-G/Bowling.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/bowling.bin\n",
            "copying boxing.bin from HC ROMS/BY ALPHABET/A-G/Boxing.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/boxing.bin\n",
            "copying breakout.bin from HC ROMS/BY ALPHABET/A-G/Breakout - Breakaway IV.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/breakout.bin\n",
            "copying carnival.bin from HC ROMS/BY ALPHABET/A-G/Carnival.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/carnival.bin\n",
            "copying centipede.bin from HC ROMS/BY ALPHABET/A-G/Centipede.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/centipede.bin\n",
            "copying chopper_command.bin from HC ROMS/BY ALPHABET/A-G/Chopper Command.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/chopper_command.bin\n",
            "copying defender.bin from HC ROMS/BY ALPHABET/A-G/Defender.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/defender.bin\n",
            "copying demon_attack.bin from HC ROMS/BY ALPHABET/A-G/Demon Attack.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/demon_attack.bin\n",
            "copying donkey_kong.bin from HC ROMS/BY ALPHABET/A-G/Donkey Kong.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/donkey_kong.bin\n",
            "copying double_dunk.bin from HC ROMS/BY ALPHABET/A-G/Double Dunk.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/double_dunk.bin\n",
            "copying enduro.bin from HC ROMS/BY ALPHABET/A-G/Enduro.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/enduro.bin\n",
            "copying fishing_derby.bin from HC ROMS/BY ALPHABET/A-G/Fishing Derby.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/fishing_derby.bin\n",
            "copying freeway.bin from HC ROMS/BY ALPHABET/A-G/Freeway.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/freeway.bin\n",
            "copying frogger.bin from HC ROMS/BY ALPHABET/A-G/Frogger.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/frogger.bin\n",
            "copying frostbite.bin from HC ROMS/BY ALPHABET/A-G/Frostbite.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/frostbite.bin\n",
            "copying galaxian.bin from HC ROMS/BY ALPHABET/A-G/Galaxian.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/galaxian.bin\n",
            "copying gopher.bin from HC ROMS/BY ALPHABET/A-G/Gopher.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/gopher.bin\n",
            "copying hero.bin from HC ROMS/BY ALPHABET/H-R/H.E.R.O..bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/hero.bin\n",
            "copying ice_hockey.bin from HC ROMS/BY ALPHABET/H-R/Ice Hockey.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/ice_hockey.bin\n",
            "copying journey_escape.bin from HC ROMS/BY ALPHABET/H-R/Journey Escape.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/journey_escape.bin\n",
            "copying kaboom.bin from HC ROMS/BY ALPHABET/H-R/Kaboom!.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/kaboom.bin\n",
            "copying kangaroo.bin from HC ROMS/BY ALPHABET/H-R/Kangaroo.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/kangaroo.bin\n",
            "copying kung_fu_master.bin from HC ROMS/BY ALPHABET/H-R/Kung-Fu Master.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/kung_fu_master.bin\n",
            "copying lost_luggage.bin from HC ROMS/BY ALPHABET/H-R/Lost Luggage [no opening scene].bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/lost_luggage.bin\n",
            "copying ms_pacman.bin from HC ROMS/BY ALPHABET/H-R/Ms. Pac-Man.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/ms_pacman.bin\n",
            "copying name_this_game.bin from HC ROMS/BY ALPHABET/H-R/Name This Game.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/name_this_game.bin\n",
            "copying phoenix.bin from HC ROMS/BY ALPHABET/H-R/Phoenix.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/phoenix.bin\n",
            "copying pitfall.bin from HC ROMS/BY ALPHABET/H-R/Pitfall! - Pitfall Harry's Jungle Adventure.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/pitfall.bin\n",
            "copying pooyan.bin from HC ROMS/BY ALPHABET/H-R/Pooyan.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/pooyan.bin\n",
            "copying private_eye.bin from HC ROMS/BY ALPHABET/H-R/Private Eye.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/private_eye.bin\n",
            "copying qbert.bin from HC ROMS/BY ALPHABET/H-R/Q-bert.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/qbert.bin\n",
            "copying riverraid.bin from HC ROMS/BY ALPHABET/H-R/River Raid.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/riverraid.bin\n",
            "copying road_runner.bin from patched version of HC ROMS/BY ALPHABET/H-R/Road Runner.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/road_runner.bin\n",
            "copying robotank.bin from HC ROMS/BY ALPHABET/H-R/Robot Tank.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/robotank.bin\n",
            "copying seaquest.bin from HC ROMS/BY ALPHABET/S-Z/Seaquest.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/seaquest.bin\n",
            "copying skiing.bin from HC ROMS/BY ALPHABET/S-Z/Skiing.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/skiing.bin\n",
            "copying solaris.bin from HC ROMS/BY ALPHABET/S-Z/Solaris.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/solaris.bin\n",
            "copying space_invaders.bin from HC ROMS/BY ALPHABET/S-Z/Space Invaders.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/space_invaders.bin\n",
            "copying surround.bin from HC ROMS/BY ALPHABET/S-Z/Surround - Chase.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/surround.bin\n",
            "copying tennis.bin from HC ROMS/BY ALPHABET/S-Z/Tennis.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/tennis.bin\n",
            "copying trondead.bin from HC ROMS/BY ALPHABET/S-Z/TRON - Deadly Discs.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/trondead.bin\n",
            "copying tutankham.bin from HC ROMS/BY ALPHABET/S-Z/Tutankham.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/tutankham.bin\n",
            "copying venture.bin from HC ROMS/BY ALPHABET/S-Z/Venture.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/venture.bin\n",
            "copying pong.bin from HC ROMS/BY ALPHABET/S-Z/Video Olympics - Pong Sports.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/pong.bin\n",
            "copying video_pinball.bin from HC ROMS/BY ALPHABET/S-Z/Video Pinball - Arcade Pinball.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/video_pinball.bin\n",
            "copying wizard_of_wor.bin from HC ROMS/BY ALPHABET/S-Z/Wizard of Wor.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/wizard_of_wor.bin\n",
            "copying yars_revenge.bin from HC ROMS/BY ALPHABET/S-Z/Yars' Revenge.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/yars_revenge.bin\n",
            "copying zaxxon.bin from HC ROMS/BY ALPHABET/S-Z/Zaxxon.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/zaxxon.bin\n",
            "copying assault.bin from HC ROMS/NTSC VERSIONS OF PAL ORIGINALS/Assault (AKA Sky Alien) (1983) (Bomb - Onbase) (CA281).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/assault.bin\n",
            "copying asterix.bin from ROMS/Asterix (AKA Taz) (07-27-1983) (Atari, Jerome Domurat, Steve Woita) (CX2696) (Prototype).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/asterix.bin\n"
          ]
        }
      ],
      "source": [
        "import urllib.request\n",
        "urllib.request.urlretrieve('http://www.atarimania.com/roms/Roms.rar','Roms.rar')\n",
        "!pip install unrar\n",
        "!unrar x Roms.rar\n",
        "!mkdir rars\n",
        "!mv HC\\ ROMS.zip   rars\n",
        "!mv ROMS.zip  rars\n",
        "!python -m atari_py.import_roms rars"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "id": "fKP0m4BwA7aW",
        "outputId": "bef4901b-1432-4df6-e7b3-9c2a4a210061"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 4 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(84, 84, 4)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# James Note: Importing deque for stack\n",
        "from collections import deque\n",
        "\n",
        "\n",
        "class Atari(object):\n",
        "    def __init__(self, game: str, resized_width: int, resized_height: int, frame_skip=4, clip_reward: bool = True):\n",
        "        \"\"\"[summary]\n",
        "\n",
        "        Args:\n",
        "            game (str): [description]\n",
        "            resized_width (int): [description]\n",
        "            resized_height (int): [description]\n",
        "            frame_skip (int, optional): [description]. Defaults to 4.\n",
        "            clip_reward (bool, optional): [description]. Defaults to True.\n",
        "        \"\"\"\n",
        "        self.env = gym.make(game)\n",
        "        self.action_space_size = self.env.action_space.n  # The number of actions an agent can perform (int)\n",
        "        self.resized_width = resized_width\n",
        "        self.resized_height = resized_height\n",
        "\n",
        "        # James Note: Added these two attrs for frame stacking\n",
        "        self._frame_stack = deque([], frame_skip)\n",
        "        self.frame_skip = frame_skip\n",
        "        self.output = None\n",
        "\n",
        "        # Also clip reward\n",
        "        self.clip_reward = clip_reward\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"[summary]\n",
        "\n",
        "        Returns:\n",
        "            [type]: [description]\n",
        "        \"\"\"\n",
        "        # Should reset the environment to the begining\n",
        "        # Returns initial state\n",
        "        # James Note: Implemented stacking from reset\n",
        "        reset_obs = self.get_preprocessed_frame(self.env.reset())\n",
        "        for _ in range(self.frame_skip):\n",
        "            self._frame_stack.append(reset_obs)\n",
        "        self.output = np.concatenate(self._frame_stack, axis=-1)\n",
        "        return self.output\n",
        "\n",
        "    def num_actions_available(self):\n",
        "        \"\"\"[summary]\n",
        "\n",
        "        Returns:\n",
        "            [type]: [description]\n",
        "        \"\"\"\n",
        "        # Return total number of actions\n",
        "        return self.env.action_space.n\n",
        "\n",
        "    def get_preprocessed_frame(self, observation):\n",
        "        \"\"\"[summary]\n",
        "\n",
        "        Args:\n",
        "            observation ([type]): [description]\n",
        "\n",
        "        Returns:\n",
        "            [type]: [description]\n",
        "        \"\"\"\n",
        "\n",
        "        # Convert image to grayscale\n",
        "        # Rescale image\n",
        "        # James Note: Rewrote this method to return a uint8 and expand the dims\n",
        "        img = observation[34:-16, :, :]\n",
        "        # Resize image\n",
        "        img = cv2.resize(img, (84,84))\n",
        "        # Grayscale\n",
        "        img = img.mean(-1,keepdims=True)\n",
        "        # Return as an unsigned integer to save space\n",
        "        # Normalization occurs upon model input\n",
        "        return img.astype(\"uint8\")\n",
        "\n",
        "\n",
        "    def get_action_meanings(self):\n",
        "        \"\"\"[summary]\n",
        "\n",
        "        Returns:\n",
        "            [type]: [description]\n",
        "        \"\"\"\n",
        "        # Prints meaings of all possible actions\n",
        "        return self.env.get_action_meanings()\n",
        "\n",
        "    def step(self, action: int, plot_frames: bool = False) -> tuple:\n",
        "        \"\"\"[summary]\n",
        "        Args:\n",
        "            action (int): [description]\n",
        "            frame_skip (int, optional): [description]. Defaults to 1.\n",
        "\n",
        "        Returns:\n",
        "            tuple: [description]\n",
        "        \"\"\"\n",
        "\n",
        "        # Note from James\n",
        "        # Frame skip was not implemented >>> Now it is (rewrote method)\n",
        "\n",
        "        total_reward = 0\n",
        "        second_to_last = None\n",
        "        last = None\n",
        "        for i in range(self.frame_skip):\n",
        "            # take a step in the env and return the next state, reward,\n",
        "            # and if the game is done as a tuple\n",
        "            observation, reward, done, info = self.env.step(action)\n",
        "\n",
        "            # Sum up the total reward\n",
        "            total_reward += reward\n",
        "\n",
        "            # Store the second to last frame\n",
        "            if i == self.frame_skip - 1:\n",
        "                last = self.get_preprocessed_frame(observation)\n",
        "            elif i == self.frame_skip - 2:\n",
        "                second_to_last = self.get_preprocessed_frame(observation)\n",
        "\n",
        "        # Get the max of the last frame\n",
        "        max_frame = np.array([second_to_last, last]).max(axis=0)\n",
        "\n",
        "        # Append the newest frame\n",
        "        self._frame_stack.append(max_frame)\n",
        "\n",
        "        # If the num of stacks is correct then we concat\n",
        "        assert len(self._frame_stack) == self.frame_skip or self.output is not None, \"Must Reset Env To Step\"\n",
        "        self.output = np.concatenate(self._frame_stack, axis=-1)\n",
        "\n",
        "        if plot_frames:\n",
        "            import matplotlib.pyplot as plt\n",
        "            f, axarr = plt.subplots(2,2)\n",
        "\n",
        "            axarr[0,0].imshow((self.output[:, :, 0] / 255.0).astype(np.float32))\n",
        "            axarr[0,0].set_title(\"Frame-1\")\n",
        "\n",
        "            axarr[0,1].imshow((self.output[:, :, 1] / 255.0).astype(np.float32))\n",
        "            axarr[0,1].set_title(\"Frame-2\")\n",
        "\n",
        "            axarr[1,0].imshow((self.output[:, :, 2] / 255.0).astype(np.float32))\n",
        "            axarr[1,0].set_title(\"Frame-3\")\n",
        "\n",
        "            axarr[1,1].imshow((self.output[:, :, 3] / 255.0).astype(np.float32))\n",
        "            axarr[1,1].set_title(\"Frame-4\")\n",
        "            plt.show()\n",
        "\n",
        "        # Clip the reward\n",
        "        if self.clip_reward:\n",
        "            total_reward = np.clip(total_reward, -1, 1)\n",
        "\n",
        "        return self.output, total_reward, done, info\n",
        "\n",
        "    def render(self):\n",
        "        # Render the game state\n",
        "        self.env.render()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    game = Atari(\"Breakout-v4\", 84, 84)\n",
        "    game.reset()\n",
        "    print(game.step(0, plot_frames=True)[0].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q3a5m1GCyN0C",
        "outputId": "eec7c975-ebaa-4ae2-dfe6-ce8d49f3a126"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow==1.14.0 in /usr/local/lib/python3.7/dist-packages (1.14.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (1.15.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (3.17.3)\n",
            "Requirement already satisfied: tensorboard<1.15.0,>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (1.14.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (0.8.1)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (0.12.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (0.2.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (1.1.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (1.13.3)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (1.0.8)\n",
            "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (1.19.5)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (0.37.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (1.42.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (1.1.2)\n",
            "Requirement already satisfied: tensorflow-estimator<1.15.0rc0,>=1.14.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (1.14.0)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (0.4.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.6->tensorflow==1.14.0) (3.1.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (57.4.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (3.3.6)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (4.8.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (3.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (3.10.0.2)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications>=1.0.6->tensorflow==1.14.0) (1.5.2)\n",
            "Requirement already satisfied: keras==2.3.1 in /usr/local/lib/python3.7/dist-packages (2.3.1)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from keras==2.3.1) (1.15.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from keras==2.3.1) (1.4.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from keras==2.3.1) (1.1.2)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras==2.3.1) (3.1.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras==2.3.1) (3.13)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.7/dist-packages (from keras==2.3.1) (1.0.8)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras==2.3.1) (1.19.5)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras==2.3.1) (1.5.2)\n",
            "Requirement already satisfied: keras-rl in /usr/local/lib/python3.7/dist-packages (0.4.2)\n",
            "Requirement already satisfied: keras>=2.0.7 in /usr/local/lib/python3.7/dist-packages (from keras-rl) (2.3.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras>=2.0.7->keras-rl) (3.13)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from keras>=2.0.7->keras-rl) (1.15.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from keras>=2.0.7->keras-rl) (1.1.2)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.7/dist-packages (from keras>=2.0.7->keras-rl) (1.0.8)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras>=2.0.7->keras-rl) (1.19.5)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras>=2.0.7->keras-rl) (3.1.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from keras>=2.0.7->keras-rl) (1.4.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras>=2.0.7->keras-rl) (1.5.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow==1.14.0\n",
        "!pip install keras==2.3.1\n",
        "!pip install keras-rl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y2JAeIcqjvda",
        "outputId": "1e0014dc-c138-4499-99cf-049d03c98160"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training for 5000000 steps ...\n",
            "Interval 1 (0 steps performed)\n",
            "10000/10000 [==============================] - 41s 4ms/step - reward: 0.0058\n",
            "57 episodes - episode_reward: 1.000 [0.000, 7.000] - ale.lives: 2.933\n",
            "\n",
            "Interval 2 (10000 steps performed)\n",
            "10000/10000 [==============================] - 42s 4ms/step - reward: 0.0071\n",
            "53 episodes - episode_reward: 1.340 [0.000, 4.000] - ale.lives: 2.971\n",
            "\n",
            "Interval 3 (20000 steps performed)\n",
            "10000/10000 [==============================] - 42s 4ms/step - reward: 0.0054\n",
            "58 episodes - episode_reward: 0.948 [0.000, 5.000] - ale.lives: 2.940\n",
            "\n",
            "Interval 4 (30000 steps performed)\n",
            "10000/10000 [==============================] - 40s 4ms/step - reward: 0.0054\n",
            "59 episodes - episode_reward: 0.915 [0.000, 3.000] - ale.lives: 2.934\n",
            "\n",
            "Interval 5 (40000 steps performed)\n",
            "10000/10000 [==============================] - 40s 4ms/step - reward: 0.0060\n",
            "57 episodes - episode_reward: 1.053 [0.000, 4.000] - ale.lives: 3.024\n",
            "\n",
            "Interval 6 (50000 steps performed)\n",
            "10000/10000 [==============================] - 40s 4ms/step - reward: 0.0057\n",
            "58 episodes - episode_reward: 0.983 [0.000, 4.000] - ale.lives: 2.894\n",
            "\n",
            "Interval 7 (60000 steps performed)\n",
            "10000/10000 [==============================] - 40s 4ms/step - reward: 0.0067\n",
            "56 episodes - episode_reward: 1.196 [0.000, 4.000] - ale.lives: 2.913\n",
            "\n",
            "Interval 8 (70000 steps performed)\n",
            "10000/10000 [==============================] - 46s 5ms/step - reward: 0.0044\n",
            "59 episodes - episode_reward: 0.746 [0.000, 4.000] - ale.lives: 2.938\n",
            "\n",
            "Interval 9 (80000 steps performed)\n",
            "10000/10000 [==============================] - 42s 4ms/step - reward: 0.0065\n",
            "56 episodes - episode_reward: 1.161 [0.000, 5.000] - ale.lives: 2.912\n",
            "\n",
            "Interval 10 (90000 steps performed)\n",
            "10000/10000 [==============================] - 41s 4ms/step - reward: 0.0058\n",
            "57 episodes - episode_reward: 1.018 [0.000, 5.000] - ale.lives: 2.933\n",
            "\n",
            "Interval 11 (100000 steps performed)\n",
            "10000/10000 [==============================] - 156s 16ms/step - reward: 0.0068\n",
            "54 episodes - episode_reward: 1.259 [0.000, 4.000] - loss: 0.003 - mae: 0.038 - mean_q: 0.040 - mean_eps: 0.906 - ale.lives: 2.971\n",
            "\n",
            "Interval 12 (110000 steps performed)\n",
            "10000/10000 [==============================] - 148s 15ms/step - reward: 0.0050\n",
            "58 episodes - episode_reward: 0.862 [0.000, 3.000] - loss: 0.001 - mae: 0.038 - mean_q: 0.052 - mean_eps: 0.897 - ale.lives: 2.995\n",
            "\n",
            "Interval 13 (120000 steps performed)\n",
            "10000/10000 [==============================] - 149s 15ms/step - reward: 0.0066\n",
            "54 episodes - episode_reward: 1.222 [0.000, 4.000] - loss: 0.001 - mae: 0.041 - mean_q: 0.056 - mean_eps: 0.888 - ale.lives: 2.924\n",
            "\n",
            "Interval 14 (130000 steps performed)\n",
            "10000/10000 [==============================] - 149s 15ms/step - reward: 0.0058\n",
            "56 episodes - episode_reward: 1.000 [0.000, 3.000] - loss: 0.001 - mae: 0.047 - mean_q: 0.065 - mean_eps: 0.879 - ale.lives: 2.941\n",
            "\n",
            "Interval 15 (140000 steps performed)\n",
            "10000/10000 [==============================] - 145s 15ms/step - reward: 0.0063\n",
            "56 episodes - episode_reward: 1.161 [0.000, 4.000] - loss: 0.000 - mae: 0.053 - mean_q: 0.073 - mean_eps: 0.870 - ale.lives: 2.911\n",
            "\n",
            "Interval 16 (150000 steps performed)\n",
            "10000/10000 [==============================] - 147s 15ms/step - reward: 0.0057\n",
            "56 episodes - episode_reward: 0.982 [0.000, 4.000] - loss: 0.001 - mae: 0.060 - mean_q: 0.082 - mean_eps: 0.861 - ale.lives: 2.921\n",
            "\n",
            "Interval 17 (160000 steps performed)\n",
            "10000/10000 [==============================] - 147s 15ms/step - reward: 0.0068\n",
            "56 episodes - episode_reward: 1.250 [0.000, 5.000] - loss: 0.000 - mae: 0.063 - mean_q: 0.086 - mean_eps: 0.852 - ale.lives: 2.968\n",
            "\n",
            "Interval 18 (170000 steps performed)\n",
            "10000/10000 [==============================] - 146s 15ms/step - reward: 0.0065\n",
            "55 episodes - episode_reward: 1.182 [0.000, 4.000] - loss: 0.000 - mae: 0.068 - mean_q: 0.093 - mean_eps: 0.843 - ale.lives: 3.008\n",
            "\n",
            "Interval 19 (180000 steps performed)\n",
            "10000/10000 [==============================] - 145s 15ms/step - reward: 0.0059\n",
            "57 episodes - episode_reward: 1.035 [0.000, 6.000] - loss: 0.000 - mae: 0.073 - mean_q: 0.099 - mean_eps: 0.834 - ale.lives: 3.016\n",
            "\n",
            "Interval 20 (190000 steps performed)\n",
            "10000/10000 [==============================] - 144s 14ms/step - reward: 0.0065\n",
            "56 episodes - episode_reward: 1.143 [0.000, 5.000] - loss: 0.000 - mae: 0.077 - mean_q: 0.104 - mean_eps: 0.825 - ale.lives: 2.930\n",
            "\n",
            "Interval 21 (200000 steps performed)\n",
            "10000/10000 [==============================] - 142s 14ms/step - reward: 0.0069\n",
            "53 episodes - episode_reward: 1.321 [0.000, 6.000] - loss: 0.000 - mae: 0.081 - mean_q: 0.110 - mean_eps: 0.816 - ale.lives: 2.897\n",
            "\n",
            "Interval 22 (210000 steps performed)\n",
            "10000/10000 [==============================] - 144s 14ms/step - reward: 0.0061\n",
            "55 episodes - episode_reward: 1.073 [0.000, 5.000] - loss: 0.000 - mae: 0.085 - mean_q: 0.115 - mean_eps: 0.807 - ale.lives: 2.998\n",
            "\n",
            "Interval 23 (220000 steps performed)\n",
            "10000/10000 [==============================] - 143s 14ms/step - reward: 0.0069\n",
            "53 episodes - episode_reward: 1.340 [0.000, 4.000] - loss: 0.000 - mae: 0.091 - mean_q: 0.122 - mean_eps: 0.798 - ale.lives: 2.967\n",
            "\n",
            "Interval 24 (230000 steps performed)\n",
            "10000/10000 [==============================] - 143s 14ms/step - reward: 0.0078\n",
            "54 episodes - episode_reward: 1.407 [0.000, 3.000] - loss: 0.000 - mae: 0.098 - mean_q: 0.132 - mean_eps: 0.789 - ale.lives: 2.892\n",
            "\n",
            "Interval 25 (240000 steps performed)\n",
            "10000/10000 [==============================] - 145s 15ms/step - reward: 0.0087\n",
            "52 episodes - episode_reward: 1.712 [0.000, 7.000] - loss: 0.001 - mae: 0.104 - mean_q: 0.139 - mean_eps: 0.780 - ale.lives: 2.924\n",
            "\n",
            "Interval 26 (250000 steps performed)\n",
            "10000/10000 [==============================] - 151s 15ms/step - reward: 0.0074\n",
            "55 episodes - episode_reward: 1.309 [0.000, 4.000] - loss: 0.001 - mae: 0.114 - mean_q: 0.152 - mean_eps: 0.771 - ale.lives: 2.958\n",
            "\n",
            "Interval 27 (260000 steps performed)\n",
            "10000/10000 [==============================] - 152s 15ms/step - reward: 0.0087\n",
            "51 episodes - episode_reward: 1.745 [0.000, 5.000] - loss: 0.001 - mae: 0.122 - mean_q: 0.163 - mean_eps: 0.762 - ale.lives: 2.970\n",
            "\n",
            "Interval 28 (270000 steps performed)\n",
            "10000/10000 [==============================] - 153s 15ms/step - reward: 0.0073\n",
            "56 episodes - episode_reward: 1.304 [0.000, 4.000] - loss: 0.001 - mae: 0.128 - mean_q: 0.171 - mean_eps: 0.753 - ale.lives: 3.018\n",
            "\n",
            "Interval 29 (280000 steps performed)\n",
            "10000/10000 [==============================] - 154s 15ms/step - reward: 0.0100\n",
            "47 episodes - episode_reward: 2.106 [0.000, 8.000] - loss: 0.001 - mae: 0.139 - mean_q: 0.189 - mean_eps: 0.744 - ale.lives: 3.093\n",
            "\n",
            "Interval 30 (290000 steps performed)\n",
            "10000/10000 [==============================] - 153s 15ms/step - reward: 0.0099\n",
            "48 episodes - episode_reward: 2.000 [0.000, 5.000] - loss: 0.001 - mae: 0.144 - mean_q: 0.197 - mean_eps: 0.735 - ale.lives: 2.877\n",
            "\n",
            "Interval 31 (300000 steps performed)\n",
            "10000/10000 [==============================] - 154s 15ms/step - reward: 0.0108\n",
            "47 episodes - episode_reward: 2.362 [0.000, 5.000] - loss: 0.001 - mae: 0.158 - mean_q: 0.218 - mean_eps: 0.726 - ale.lives: 2.946\n",
            "\n",
            "Interval 32 (310000 steps performed)\n",
            "10000/10000 [==============================] - 153s 15ms/step - reward: 0.0099\n",
            "46 episodes - episode_reward: 2.130 [0.000, 7.000] - loss: 0.001 - mae: 0.167 - mean_q: 0.230 - mean_eps: 0.717 - ale.lives: 3.038\n",
            "\n",
            "Interval 33 (320000 steps performed)\n",
            "10000/10000 [==============================] - 152s 15ms/step - reward: 0.0123\n",
            "41 episodes - episode_reward: 3.049 [1.000, 9.000] - loss: 0.001 - mae: 0.181 - mean_q: 0.253 - mean_eps: 0.708 - ale.lives: 2.922\n",
            "\n",
            "Interval 34 (330000 steps performed)\n",
            "10000/10000 [==============================] - 152s 15ms/step - reward: 0.0121\n",
            "42 episodes - episode_reward: 2.881 [0.000, 5.000] - loss: 0.001 - mae: 0.201 - mean_q: 0.282 - mean_eps: 0.699 - ale.lives: 2.982\n",
            "\n",
            "Interval 35 (340000 steps performed)\n",
            "10000/10000 [==============================] - 153s 15ms/step - reward: 0.0117\n",
            "43 episodes - episode_reward: 2.674 [1.000, 6.000] - loss: 0.001 - mae: 0.224 - mean_q: 0.316 - mean_eps: 0.690 - ale.lives: 3.002\n",
            "\n",
            "Interval 36 (350000 steps performed)\n",
            "10000/10000 [==============================] - 153s 15ms/step - reward: 0.0138\n",
            "35 episodes - episode_reward: 3.943 [1.000, 11.000] - loss: 0.001 - mae: 0.244 - mean_q: 0.345 - mean_eps: 0.681 - ale.lives: 3.089\n",
            "\n",
            "Interval 37 (360000 steps performed)\n",
            "10000/10000 [==============================] - 153s 15ms/step - reward: 0.0134\n",
            "35 episodes - episode_reward: 3.857 [1.000, 9.000] - loss: 0.001 - mae: 0.269 - mean_q: 0.383 - mean_eps: 0.672 - ale.lives: 3.079\n",
            "\n",
            "Interval 38 (370000 steps performed)\n",
            "10000/10000 [==============================] - 154s 15ms/step - reward: 0.0140\n",
            "33 episodes - episode_reward: 4.242 [0.000, 12.000] - loss: 0.002 - mae: 0.303 - mean_q: 0.433 - mean_eps: 0.663 - ale.lives: 3.012\n",
            "\n",
            "Interval 39 (380000 steps performed)\n",
            "10000/10000 [==============================] - 153s 15ms/step - reward: 0.0139\n",
            "33 episodes - episode_reward: 4.242 [0.000, 8.000] - loss: 0.002 - mae: 0.331 - mean_q: 0.470 - mean_eps: 0.654 - ale.lives: 2.972\n",
            "\n",
            "Interval 40 (390000 steps performed)\n",
            "10000/10000 [==============================] - 153s 15ms/step - reward: 0.0126\n",
            "34 episodes - episode_reward: 3.588 [0.000, 8.000] - loss: 0.002 - mae: 0.350 - mean_q: 0.495 - mean_eps: 0.645 - ale.lives: 3.017\n",
            "\n",
            "Interval 41 (400000 steps performed)\n",
            "10000/10000 [==============================] - 152s 15ms/step - reward: 0.0129\n",
            "36 episodes - episode_reward: 3.639 [1.000, 11.000] - loss: 0.002 - mae: 0.375 - mean_q: 0.529 - mean_eps: 0.636 - ale.lives: 3.006\n",
            "\n",
            "Interval 42 (410000 steps performed)\n",
            "10000/10000 [==============================] - 153s 15ms/step - reward: 0.0142\n",
            "35 episodes - episode_reward: 4.000 [0.000, 9.000] - loss: 0.002 - mae: 0.400 - mean_q: 0.569 - mean_eps: 0.627 - ale.lives: 2.928\n",
            "\n",
            "Interval 43 (420000 steps performed)\n",
            "10000/10000 [==============================] - 153s 15ms/step - reward: 0.0143\n",
            "34 episodes - episode_reward: 4.206 [1.000, 11.000] - loss: 0.004 - mae: 0.435 - mean_q: 0.633 - mean_eps: 0.618 - ale.lives: 3.122\n",
            "\n",
            "Interval 44 (430000 steps performed)\n",
            "10000/10000 [==============================] - 153s 15ms/step - reward: 0.0146\n",
            "35 episodes - episode_reward: 4.143 [0.000, 11.000] - loss: 0.004 - mae: 0.480 - mean_q: 0.687 - mean_eps: 0.609 - ale.lives: 2.974\n",
            "\n",
            "Interval 45 (440000 steps performed)\n",
            "10000/10000 [==============================] - 170s 17ms/step - reward: 0.0142\n",
            "33 episodes - episode_reward: 4.333 [1.000, 9.000] - loss: 0.004 - mae: 0.520 - mean_q: 0.732 - mean_eps: 0.600 - ale.lives: 2.843\n",
            "\n",
            "Interval 46 (450000 steps performed)\n",
            "10000/10000 [==============================] - 178s 18ms/step - reward: 0.0148\n",
            "34 episodes - episode_reward: 4.412 [0.000, 11.000] - loss: 0.004 - mae: 0.560 - mean_q: 0.786 - mean_eps: 0.591 - ale.lives: 3.073\n",
            "\n",
            "Interval 47 (460000 steps performed)\n",
            "10000/10000 [==============================] - 179s 18ms/step - reward: 0.0134\n",
            "37 episodes - episode_reward: 3.649 [0.000, 11.000] - loss: 0.004 - mae: 0.597 - mean_q: 0.834 - mean_eps: 0.582 - ale.lives: 2.879\n",
            "\n",
            "Interval 48 (470000 steps performed)\n",
            "10000/10000 [==============================] - 178s 18ms/step - reward: 0.0131\n",
            "36 episodes - episode_reward: 3.667 [0.000, 10.000] - loss: 0.004 - mae: 0.627 - mean_q: 0.871 - mean_eps: 0.573 - ale.lives: 2.997\n",
            "\n",
            "Interval 49 (480000 steps performed)\n",
            "10000/10000 [==============================] - 179s 18ms/step - reward: 0.0127\n",
            "37 episodes - episode_reward: 3.297 [0.000, 8.000] - loss: 0.003 - mae: 0.656 - mean_q: 0.904 - mean_eps: 0.564 - ale.lives: 2.780\n",
            "\n",
            "Interval 50 (490000 steps performed)\n",
            "10000/10000 [==============================] - 177s 18ms/step - reward: 0.0126\n",
            "38 episodes - episode_reward: 3.447 [0.000, 11.000] - loss: 0.003 - mae: 0.678 - mean_q: 0.930 - mean_eps: 0.555 - ale.lives: 2.882\n",
            "\n",
            "Interval 51 (500000 steps performed)\n",
            "10000/10000 [==============================] - 179s 18ms/step - reward: 0.0136\n",
            "33 episodes - episode_reward: 3.879 [0.000, 8.000] - loss: 0.003 - mae: 0.704 - mean_q: 0.962 - mean_eps: 0.546 - ale.lives: 2.941\n",
            "\n",
            "Interval 52 (510000 steps performed)\n",
            "10000/10000 [==============================] - 178s 18ms/step - reward: 0.0153\n",
            "29 episodes - episode_reward: 5.276 [0.000, 9.000] - loss: 0.003 - mae: 0.719 - mean_q: 0.978 - mean_eps: 0.537 - ale.lives: 2.963\n",
            "\n",
            "Interval 53 (520000 steps performed)\n",
            "10000/10000 [==============================] - 185s 19ms/step - reward: 0.0145\n",
            "34 episodes - episode_reward: 4.500 [0.000, 11.000] - loss: 0.002 - mae: 0.727 - mean_q: 0.986 - mean_eps: 0.528 - ale.lives: 2.997\n",
            "\n",
            "Interval 54 (530000 steps performed)\n",
            "10000/10000 [==============================] - 181s 18ms/step - reward: 0.0146\n",
            "31 episodes - episode_reward: 4.645 [0.000, 10.000] - loss: 0.002 - mae: 0.741 - mean_q: 1.002 - mean_eps: 0.519 - ale.lives: 2.955\n",
            "\n",
            "Interval 55 (540000 steps performed)\n",
            "10000/10000 [==============================] - 178s 18ms/step - reward: 0.0157\n",
            "32 episodes - episode_reward: 4.938 [2.000, 10.000] - loss: 0.002 - mae: 0.756 - mean_q: 1.022 - mean_eps: 0.510 - ale.lives: 2.817\n",
            "\n",
            "Interval 56 (550000 steps performed)\n",
            "10000/10000 [==============================] - 180s 18ms/step - reward: 0.0170\n",
            "27 episodes - episode_reward: 6.296 [0.000, 12.000] - loss: 0.003 - mae: 0.771 - mean_q: 1.044 - mean_eps: 0.501 - ale.lives: 2.972\n",
            "\n",
            "Interval 57 (560000 steps performed)\n",
            "10000/10000 [==============================] - 180s 18ms/step - reward: 0.0179\n",
            "27 episodes - episode_reward: 6.407 [1.000, 13.000] - loss: 0.002 - mae: 0.783 - mean_q: 1.058 - mean_eps: 0.492 - ale.lives: 3.052\n",
            "\n",
            "Interval 58 (570000 steps performed)\n",
            "10000/10000 [==============================] - 180s 18ms/step - reward: 0.0174\n",
            "28 episodes - episode_reward: 6.321 [2.000, 12.000] - loss: 0.002 - mae: 0.791 - mean_q: 1.068 - mean_eps: 0.483 - ale.lives: 3.081\n",
            "\n",
            "Interval 59 (580000 steps performed)\n",
            "10000/10000 [==============================] - 182s 18ms/step - reward: 0.0180\n",
            "25 episodes - episode_reward: 7.360 [1.000, 15.000] - loss: 0.002 - mae: 0.805 - mean_q: 1.085 - mean_eps: 0.474 - ale.lives: 3.028\n",
            "\n",
            "Interval 60 (590000 steps performed)\n",
            "10000/10000 [==============================] - 180s 18ms/step - reward: 0.0174\n",
            "28 episodes - episode_reward: 6.036 [0.000, 11.000] - loss: 0.002 - mae: 0.804 - mean_q: 1.083 - mean_eps: 0.465 - ale.lives: 3.020\n",
            "\n",
            "Interval 61 (600000 steps performed)\n",
            "10000/10000 [==============================] - 181s 18ms/step - reward: 0.0163\n",
            "30 episodes - episode_reward: 5.500 [1.000, 12.000] - loss: 0.002 - mae: 0.809 - mean_q: 1.089 - mean_eps: 0.456 - ale.lives: 2.894\n",
            "\n",
            "Interval 62 (610000 steps performed)\n",
            "10000/10000 [==============================] - 179s 18ms/step - reward: 0.0164\n",
            "28 episodes - episode_reward: 5.857 [1.000, 12.000] - loss: 0.002 - mae: 0.818 - mean_q: 1.102 - mean_eps: 0.447 - ale.lives: 2.848\n",
            "\n",
            "Interval 63 (620000 steps performed)\n",
            "10000/10000 [==============================] - 176s 18ms/step - reward: 0.0173\n",
            "27 episodes - episode_reward: 6.481 [0.000, 12.000] - loss: 0.002 - mae: 0.825 - mean_q: 1.111 - mean_eps: 0.438 - ale.lives: 3.020\n",
            "\n",
            "Interval 64 (630000 steps performed)\n",
            "10000/10000 [==============================] - 178s 18ms/step - reward: 0.0185\n",
            "23 episodes - episode_reward: 7.739 [2.000, 14.000] - loss: 0.002 - mae: 0.839 - mean_q: 1.131 - mean_eps: 0.429 - ale.lives: 2.947\n",
            "\n",
            "Interval 65 (640000 steps performed)\n",
            "10000/10000 [==============================] - 178s 18ms/step - reward: 0.0179\n",
            "27 episodes - episode_reward: 6.889 [0.000, 13.000] - loss: 0.002 - mae: 0.855 - mean_q: 1.151 - mean_eps: 0.420 - ale.lives: 2.848\n",
            "\n",
            "Interval 66 (650000 steps performed)\n",
            "10000/10000 [==============================] - 178s 18ms/step - reward: 0.0184\n",
            "24 episodes - episode_reward: 7.375 [0.000, 14.000] - loss: 0.002 - mae: 0.862 - mean_q: 1.161 - mean_eps: 0.411 - ale.lives: 2.789\n",
            "\n",
            "Interval 67 (660000 steps performed)\n",
            "10000/10000 [==============================] - 179s 18ms/step - reward: 0.0193\n",
            "23 episodes - episode_reward: 8.478 [2.000, 16.000] - loss: 0.002 - mae: 0.867 - mean_q: 1.168 - mean_eps: 0.402 - ale.lives: 2.971\n",
            "\n",
            "Interval 68 (670000 steps performed)\n",
            "10000/10000 [==============================] - 176s 18ms/step - reward: 0.0197\n",
            "22 episodes - episode_reward: 9.182 [3.000, 17.000] - loss: 0.002 - mae: 0.874 - mean_q: 1.178 - mean_eps: 0.393 - ale.lives: 3.202\n",
            "\n",
            "Interval 69 (680000 steps performed)\n",
            "10000/10000 [==============================] - 179s 18ms/step - reward: 0.0190\n",
            "21 episodes - episode_reward: 9.095 [4.000, 15.000] - loss: 0.002 - mae: 0.879 - mean_q: 1.186 - mean_eps: 0.384 - ale.lives: 3.077\n",
            "\n",
            "Interval 70 (690000 steps performed)\n",
            "10000/10000 [==============================] - 177s 18ms/step - reward: 0.0191\n",
            "22 episodes - episode_reward: 8.545 [2.000, 17.000] - loss: 0.002 - mae: 0.894 - mean_q: 1.207 - mean_eps: 0.375 - ale.lives: 3.089\n",
            "\n",
            "Interval 71 (700000 steps performed)\n",
            "10000/10000 [==============================] - 178s 18ms/step - reward: 0.0200\n",
            "21 episodes - episode_reward: 9.476 [2.000, 15.000] - loss: 0.002 - mae: 0.914 - mean_q: 1.234 - mean_eps: 0.366 - ale.lives: 3.199\n",
            "\n",
            "Interval 72 (710000 steps performed)\n",
            "10000/10000 [==============================] - 178s 18ms/step - reward: 0.0216\n",
            "19 episodes - episode_reward: 11.000 [2.000, 18.000] - loss: 0.002 - mae: 0.922 - mean_q: 1.246 - mean_eps: 0.357 - ale.lives: 2.979\n",
            "\n",
            "Interval 73 (720000 steps performed)\n",
            "10000/10000 [==============================] - 177s 18ms/step - reward: 0.0207\n",
            "20 episodes - episode_reward: 10.550 [3.000, 22.000] - loss: 0.002 - mae: 0.926 - mean_q: 1.253 - mean_eps: 0.348 - ale.lives: 3.058\n",
            "\n",
            "Interval 74 (730000 steps performed)\n",
            "10000/10000 [==============================] - 178s 18ms/step - reward: 0.0214\n",
            "20 episodes - episode_reward: 11.000 [4.000, 18.000] - loss: 0.002 - mae: 0.940 - mean_q: 1.272 - mean_eps: 0.339 - ale.lives: 3.126\n",
            "\n",
            "Interval 75 (740000 steps performed)\n",
            "10000/10000 [==============================] - 176s 18ms/step - reward: 0.0215\n",
            "18 episodes - episode_reward: 11.778 [5.000, 18.000] - loss: 0.002 - mae: 0.956 - mean_q: 1.293 - mean_eps: 0.330 - ale.lives: 3.232\n",
            "\n",
            "Interval 76 (750000 steps performed)\n",
            "10000/10000 [==============================] - 177s 18ms/step - reward: 0.0216\n",
            "20 episodes - episode_reward: 10.700 [3.000, 18.000] - loss: 0.003 - mae: 0.976 - mean_q: 1.321 - mean_eps: 0.321 - ale.lives: 3.080\n",
            "\n",
            "Interval 77 (760000 steps performed)\n",
            "10000/10000 [==============================] - 177s 18ms/step - reward: 0.0215\n",
            "18 episodes - episode_reward: 11.611 [5.000, 16.000] - loss: 0.003 - mae: 0.990 - mean_q: 1.339 - mean_eps: 0.312 - ale.lives: 2.913\n",
            "\n",
            "Interval 78 (770000 steps performed)\n",
            "10000/10000 [==============================] - 180s 18ms/step - reward: 0.0218\n",
            "19 episodes - episode_reward: 12.000 [5.000, 25.000] - loss: 0.003 - mae: 1.010 - mean_q: 1.367 - mean_eps: 0.303 - ale.lives: 2.876\n",
            "\n",
            "Interval 79 (780000 steps performed)\n",
            "10000/10000 [==============================] - 184s 18ms/step - reward: 0.0229\n",
            "17 episodes - episode_reward: 13.588 [9.000, 18.000] - loss: 0.003 - mae: 1.027 - mean_q: 1.389 - mean_eps: 0.294 - ale.lives: 2.970\n",
            "\n",
            "Interval 80 (790000 steps performed)\n",
            "10000/10000 [==============================] - 178s 18ms/step - reward: 0.0224\n",
            "18 episodes - episode_reward: 12.278 [5.000, 17.000] - loss: 0.003 - mae: 1.044 - mean_q: 1.413 - mean_eps: 0.285 - ale.lives: 3.133\n",
            "\n",
            "Interval 81 (800000 steps performed)\n",
            "10000/10000 [==============================] - 179s 18ms/step - reward: 0.0225\n",
            "16 episodes - episode_reward: 13.500 [1.000, 23.000] - loss: 0.003 - mae: 1.057 - mean_q: 1.432 - mean_eps: 0.276 - ale.lives: 3.192\n",
            "\n",
            "Interval 82 (810000 steps performed)\n",
            "10000/10000 [==============================] - 177s 18ms/step - reward: 0.0222\n",
            "17 episodes - episode_reward: 13.588 [7.000, 21.000] - loss: 0.003 - mae: 1.072 - mean_q: 1.451 - mean_eps: 0.267 - ale.lives: 3.093\n",
            "\n",
            "Interval 83 (820000 steps performed)\n",
            "10000/10000 [==============================] - 178s 18ms/step - reward: 0.0223\n",
            "17 episodes - episode_reward: 12.882 [2.000, 20.000] - loss: 0.003 - mae: 1.091 - mean_q: 1.476 - mean_eps: 0.258 - ale.lives: 3.115\n",
            "\n",
            "Interval 84 (830000 steps performed)\n",
            "10000/10000 [==============================] - 181s 18ms/step - reward: 0.0228\n",
            "16 episodes - episode_reward: 14.312 [7.000, 23.000] - loss: 0.003 - mae: 1.102 - mean_q: 1.490 - mean_eps: 0.249 - ale.lives: 3.017\n",
            "\n",
            "Interval 85 (840000 steps performed)\n",
            "10000/10000 [==============================] - 179s 18ms/step - reward: 0.0234\n",
            "15 episodes - episode_reward: 16.000 [8.000, 22.000] - loss: 0.003 - mae: 1.118 - mean_q: 1.512 - mean_eps: 0.240 - ale.lives: 3.112\n",
            "\n",
            "Interval 86 (850000 steps performed)\n",
            "10000/10000 [==============================] - 181s 18ms/step - reward: 0.0233\n",
            "14 episodes - episode_reward: 16.071 [8.000, 23.000] - loss: 0.003 - mae: 1.126 - mean_q: 1.524 - mean_eps: 0.231 - ale.lives: 3.141\n",
            "\n",
            "Interval 87 (860000 steps performed)\n",
            "10000/10000 [==============================] - 177s 18ms/step - reward: 0.0243\n",
            "15 episodes - episode_reward: 16.333 [9.000, 23.000] - loss: 0.003 - mae: 1.143 - mean_q: 1.547 - mean_eps: 0.222 - ale.lives: 2.994\n",
            "\n",
            "Interval 88 (870000 steps performed)\n",
            "10000/10000 [==============================] - 179s 18ms/step - reward: 0.0232\n",
            "15 episodes - episode_reward: 14.933 [11.000, 20.000] - loss: 0.003 - mae: 1.152 - mean_q: 1.558 - mean_eps: 0.213 - ale.lives: 3.121\n",
            "\n",
            "Interval 89 (880000 steps performed)\n",
            "10000/10000 [==============================] - 179s 18ms/step - reward: 0.0234\n",
            "16 episodes - episode_reward: 15.312 [4.000, 22.000] - loss: 0.003 - mae: 1.157 - mean_q: 1.565 - mean_eps: 0.204 - ale.lives: 3.129\n",
            "\n",
            "Interval 90 (890000 steps performed)\n",
            "10000/10000 [==============================] - 180s 18ms/step - reward: 0.0239\n",
            "14 episodes - episode_reward: 16.643 [11.000, 26.000] - loss: 0.003 - mae: 1.166 - mean_q: 1.574 - mean_eps: 0.195 - ale.lives: 3.224\n",
            "\n",
            "Interval 91 (900000 steps performed)\n",
            "10000/10000 [==============================] - 178s 18ms/step - reward: 0.0238\n",
            "16 episodes - episode_reward: 15.125 [7.000, 22.000] - loss: 0.003 - mae: 1.181 - mean_q: 1.593 - mean_eps: 0.186 - ale.lives: 3.019\n",
            "\n",
            "Interval 92 (910000 steps performed)\n",
            "10000/10000 [==============================] - 179s 18ms/step - reward: 0.0242\n",
            "13 episodes - episode_reward: 19.000 [13.000, 24.000] - loss: 0.003 - mae: 1.195 - mean_q: 1.612 - mean_eps: 0.177 - ale.lives: 2.996\n",
            "\n",
            "Interval 93 (920000 steps performed)\n",
            "10000/10000 [==============================] - 178s 18ms/step - reward: 0.0245\n",
            "13 episodes - episode_reward: 17.692 [10.000, 24.000] - loss: 0.003 - mae: 1.206 - mean_q: 1.626 - mean_eps: 0.168 - ale.lives: 2.943\n",
            "\n",
            "Interval 94 (930000 steps performed)\n",
            "10000/10000 [==============================] - 177s 18ms/step - reward: 0.0242\n",
            "14 episodes - episode_reward: 17.357 [11.000, 26.000] - loss: 0.003 - mae: 1.215 - mean_q: 1.635 - mean_eps: 0.159 - ale.lives: 3.059\n",
            "\n",
            "Interval 95 (940000 steps performed)\n",
            "10000/10000 [==============================] - 181s 18ms/step - reward: 0.0239\n",
            "13 episodes - episode_reward: 19.308 [16.000, 29.000] - loss: 0.003 - mae: 1.227 - mean_q: 1.652 - mean_eps: 0.150 - ale.lives: 3.129\n",
            "\n",
            "Interval 96 (950000 steps performed)\n",
            "10000/10000 [==============================] - 182s 18ms/step - reward: 0.0240\n",
            "13 episodes - episode_reward: 18.231 [11.000, 28.000] - loss: 0.003 - mae: 1.235 - mean_q: 1.663 - mean_eps: 0.141 - ale.lives: 3.183\n",
            "\n",
            "Interval 97 (960000 steps performed)\n",
            "10000/10000 [==============================] - 181s 18ms/step - reward: 0.0245\n",
            "12 episodes - episode_reward: 19.167 [13.000, 27.000] - loss: 0.003 - mae: 1.247 - mean_q: 1.678 - mean_eps: 0.132 - ale.lives: 3.100\n",
            "\n",
            "Interval 98 (970000 steps performed)\n",
            "10000/10000 [==============================] - 182s 18ms/step - reward: 0.0251\n",
            "14 episodes - episode_reward: 18.000 [12.000, 26.000] - loss: 0.003 - mae: 1.260 - mean_q: 1.695 - mean_eps: 0.123 - ale.lives: 3.066\n",
            "\n",
            "Interval 99 (980000 steps performed)\n",
            "10000/10000 [==============================] - 183s 18ms/step - reward: 0.0251\n",
            "13 episodes - episode_reward: 20.538 [13.000, 25.000] - loss: 0.003 - mae: 1.268 - mean_q: 1.707 - mean_eps: 0.114 - ale.lives: 2.963\n",
            "\n",
            "Interval 100 (990000 steps performed)\n",
            "10000/10000 [==============================] - 182s 18ms/step - reward: 0.0248\n",
            "12 episodes - episode_reward: 20.583 [14.000, 28.000] - loss: 0.003 - mae: 1.276 - mean_q: 1.716 - mean_eps: 0.105 - ale.lives: 3.163\n",
            "\n",
            "Interval 101 (1000000 steps performed)\n",
            "10000/10000 [==============================] - 180s 18ms/step - reward: 0.0250\n",
            "12 episodes - episode_reward: 19.750 [14.000, 25.000] - loss: 0.003 - mae: 1.273 - mean_q: 1.712 - mean_eps: 0.100 - ale.lives: 2.992\n",
            "\n",
            "Interval 102 (1010000 steps performed)\n",
            "10000/10000 [==============================] - 181s 18ms/step - reward: 0.0249\n",
            "13 episodes - episode_reward: 20.308 [14.000, 33.000] - loss: 0.003 - mae: 1.276 - mean_q: 1.716 - mean_eps: 0.100 - ale.lives: 3.188\n",
            "\n",
            "Interval 103 (1020000 steps performed)\n",
            "10000/10000 [==============================] - 181s 18ms/step - reward: 0.0245\n",
            "12 episodes - episode_reward: 19.833 [12.000, 32.000] - loss: 0.003 - mae: 1.285 - mean_q: 1.727 - mean_eps: 0.100 - ale.lives: 3.350\n",
            "\n",
            "Interval 104 (1030000 steps performed)\n",
            "10000/10000 [==============================] - 182s 18ms/step - reward: 0.0253\n",
            "13 episodes - episode_reward: 19.615 [14.000, 26.000] - loss: 0.003 - mae: 1.285 - mean_q: 1.727 - mean_eps: 0.100 - ale.lives: 3.211\n",
            "\n",
            "Interval 105 (1040000 steps performed)\n",
            "10000/10000 [==============================] - 176s 18ms/step - reward: 0.0242\n",
            "14 episodes - episode_reward: 17.071 [6.000, 24.000] - loss: 0.003 - mae: 1.297 - mean_q: 1.743 - mean_eps: 0.100 - ale.lives: 3.028\n",
            "\n",
            "Interval 106 (1050000 steps performed)\n",
            "10000/10000 [==============================] - 173s 17ms/step - reward: 0.0248\n",
            "13 episodes - episode_reward: 18.308 [10.000, 26.000] - loss: 0.003 - mae: 1.304 - mean_q: 1.753 - mean_eps: 0.100 - ale.lives: 2.977\n",
            "\n",
            "Interval 107 (1060000 steps performed)\n",
            "10000/10000 [==============================] - 169s 17ms/step - reward: 0.0247\n",
            "12 episodes - episode_reward: 21.333 [13.000, 29.000] - loss: 0.003 - mae: 1.318 - mean_q: 1.773 - mean_eps: 0.100 - ale.lives: 3.235\n",
            "\n",
            "Interval 108 (1070000 steps performed)\n",
            "10000/10000 [==============================] - 173s 17ms/step - reward: 0.0252\n",
            "11 episodes - episode_reward: 23.455 [17.000, 30.000] - loss: 0.003 - mae: 1.323 - mean_q: 1.780 - mean_eps: 0.100 - ale.lives: 3.171\n",
            "\n",
            "Interval 109 (1080000 steps performed)\n",
            "10000/10000 [==============================] - 174s 17ms/step - reward: 0.0246\n",
            "11 episodes - episode_reward: 20.636 [16.000, 26.000] - loss: 0.003 - mae: 1.325 - mean_q: 1.782 - mean_eps: 0.100 - ale.lives: 3.083\n",
            "\n",
            "Interval 110 (1090000 steps performed)\n",
            "10000/10000 [==============================] - 174s 17ms/step - reward: 0.0254\n",
            "11 episodes - episode_reward: 23.909 [16.000, 34.000] - loss: 0.003 - mae: 1.334 - mean_q: 1.794 - mean_eps: 0.100 - ale.lives: 3.174\n",
            "\n",
            "Interval 111 (1100000 steps performed)\n",
            "10000/10000 [==============================] - 175s 17ms/step - reward: 0.0250\n",
            "13 episodes - episode_reward: 20.077 [11.000, 35.000] - loss: 0.003 - mae: 1.335 - mean_q: 1.795 - mean_eps: 0.100 - ale.lives: 3.013\n",
            "\n",
            "Interval 112 (1110000 steps performed)\n",
            "10000/10000 [==============================] - 174s 17ms/step - reward: 0.0256\n",
            "12 episodes - episode_reward: 21.667 [16.000, 29.000] - loss: 0.003 - mae: 1.344 - mean_q: 1.808 - mean_eps: 0.100 - ale.lives: 3.060\n",
            "\n",
            "Interval 113 (1120000 steps performed)\n",
            "10000/10000 [==============================] - 175s 17ms/step - reward: 0.0252\n",
            "11 episodes - episode_reward: 21.364 [12.000, 32.000] - loss: 0.003 - mae: 1.358 - mean_q: 1.826 - mean_eps: 0.100 - ale.lives: 3.211\n",
            "\n",
            "Interval 114 (1130000 steps performed)\n",
            "10000/10000 [==============================] - 174s 17ms/step - reward: 0.0248\n",
            "13 episodes - episode_reward: 19.154 [11.000, 28.000] - loss: 0.003 - mae: 1.374 - mean_q: 1.848 - mean_eps: 0.100 - ale.lives: 3.081\n",
            "\n",
            "Interval 115 (1140000 steps performed)\n",
            "10000/10000 [==============================] - 176s 18ms/step - reward: 0.0250\n",
            "13 episodes - episode_reward: 20.308 [12.000, 30.000] - loss: 0.003 - mae: 1.385 - mean_q: 1.863 - mean_eps: 0.100 - ale.lives: 3.076\n",
            "\n",
            "Interval 116 (1150000 steps performed)\n",
            "10000/10000 [==============================] - 175s 17ms/step - reward: 0.0256\n",
            "12 episodes - episode_reward: 21.000 [12.000, 31.000] - loss: 0.003 - mae: 1.390 - mean_q: 1.869 - mean_eps: 0.100 - ale.lives: 3.342\n",
            "\n",
            "Interval 117 (1160000 steps performed)\n",
            "10000/10000 [==============================] - 174s 17ms/step - reward: 0.0251\n",
            "12 episodes - episode_reward: 20.417 [11.000, 28.000] - loss: 0.003 - mae: 1.395 - mean_q: 1.876 - mean_eps: 0.100 - ale.lives: 3.019\n",
            "\n",
            "Interval 118 (1170000 steps performed)\n",
            "10000/10000 [==============================] - 175s 18ms/step - reward: 0.0250\n",
            "11 episodes - episode_reward: 22.000 [14.000, 28.000] - loss: 0.003 - mae: 1.399 - mean_q: 1.880 - mean_eps: 0.100 - ale.lives: 3.083\n",
            "\n",
            "Interval 119 (1180000 steps performed)\n",
            "10000/10000 [==============================] - 176s 18ms/step - reward: 0.0259\n",
            "12 episodes - episode_reward: 22.417 [16.000, 34.000] - loss: 0.003 - mae: 1.413 - mean_q: 1.899 - mean_eps: 0.100 - ale.lives: 3.327\n",
            "\n",
            "Interval 120 (1190000 steps performed)\n",
            "10000/10000 [==============================] - 177s 18ms/step - reward: 0.0250\n",
            "12 episodes - episode_reward: 20.417 [13.000, 31.000] - loss: 0.003 - mae: 1.417 - mean_q: 1.905 - mean_eps: 0.100 - ale.lives: 3.183\n",
            "\n",
            "Interval 121 (1200000 steps performed)\n",
            "10000/10000 [==============================] - 175s 17ms/step - reward: 0.0252\n",
            "12 episodes - episode_reward: 21.000 [11.000, 31.000] - loss: 0.003 - mae: 1.431 - mean_q: 1.924 - mean_eps: 0.100 - ale.lives: 3.201\n",
            "\n",
            "Interval 122 (1210000 steps performed)\n",
            "10000/10000 [==============================] - 176s 18ms/step - reward: 0.0249\n",
            "13 episodes - episode_reward: 20.077 [13.000, 27.000] - loss: 0.003 - mae: 1.445 - mean_q: 1.940 - mean_eps: 0.100 - ale.lives: 3.313\n",
            "\n",
            "Interval 123 (1220000 steps performed)\n",
            "10000/10000 [==============================] - 175s 18ms/step - reward: 0.0255\n",
            "11 episodes - episode_reward: 23.182 [16.000, 31.000] - loss: 0.003 - mae: 1.446 - mean_q: 1.944 - mean_eps: 0.100 - ale.lives: 3.217\n",
            "\n",
            "Interval 124 (1230000 steps performed)\n",
            "10000/10000 [==============================] - 176s 18ms/step - reward: 0.0255\n",
            "10 episodes - episode_reward: 23.100 [15.000, 32.000] - loss: 0.003 - mae: 1.459 - mean_q: 1.960 - mean_eps: 0.100 - ale.lives: 3.260\n",
            "\n",
            "Interval 125 (1240000 steps performed)\n",
            "10000/10000 [==============================] - 174s 17ms/step - reward: 0.0257\n",
            "10 episodes - episode_reward: 26.400 [19.000, 39.000] - loss: 0.003 - mae: 1.463 - mean_q: 1.964 - mean_eps: 0.100 - ale.lives: 3.415\n",
            "\n",
            "Interval 126 (1250000 steps performed)\n",
            "10000/10000 [==============================] - 178s 18ms/step - reward: 0.0251\n",
            "11 episodes - episode_reward: 22.909 [16.000, 34.000] - loss: 0.003 - mae: 1.472 - mean_q: 1.977 - mean_eps: 0.100 - ale.lives: 3.674\n",
            "\n",
            "Interval 127 (1260000 steps performed)\n",
            "10000/10000 [==============================] - 173s 17ms/step - reward: 0.0262\n",
            "12 episodes - episode_reward: 22.417 [17.000, 26.000] - loss: 0.003 - mae: 1.481 - mean_q: 1.989 - mean_eps: 0.100 - ale.lives: 3.179\n",
            "\n",
            "Interval 128 (1270000 steps performed)\n",
            "10000/10000 [==============================] - 173s 17ms/step - reward: 0.0259\n",
            "10 episodes - episode_reward: 24.900 [18.000, 31.000] - loss: 0.003 - mae: 1.490 - mean_q: 2.001 - mean_eps: 0.100 - ale.lives: 3.505\n",
            "\n",
            "Interval 129 (1280000 steps performed)\n",
            "10000/10000 [==============================] - 173s 17ms/step - reward: 0.0256\n",
            "12 episodes - episode_reward: 22.583 [15.000, 29.000] - loss: 0.003 - mae: 1.491 - mean_q: 2.004 - mean_eps: 0.100 - ale.lives: 3.146\n",
            "\n",
            "Interval 130 (1290000 steps performed)\n",
            "10000/10000 [==============================] - 174s 17ms/step - reward: 0.0257\n",
            "11 episodes - episode_reward: 23.455 [16.000, 29.000] - loss: 0.003 - mae: 1.492 - mean_q: 2.005 - mean_eps: 0.100 - ale.lives: 3.153\n",
            "\n",
            "Interval 131 (1300000 steps performed)\n",
            "10000/10000 [==============================] - 172s 17ms/step - reward: 0.0253\n",
            "12 episodes - episode_reward: 21.500 [14.000, 30.000] - loss: 0.003 - mae: 1.494 - mean_q: 2.007 - mean_eps: 0.100 - ale.lives: 3.250\n",
            "\n",
            "Interval 132 (1310000 steps performed)\n",
            "10000/10000 [==============================] - 172s 17ms/step - reward: 0.0253\n",
            "11 episodes - episode_reward: 22.182 [13.000, 32.000] - loss: 0.003 - mae: 1.495 - mean_q: 2.009 - mean_eps: 0.100 - ale.lives: 3.388\n",
            "\n",
            "Interval 133 (1320000 steps performed)\n",
            "10000/10000 [==============================] - 173s 17ms/step - reward: 0.0248\n",
            "12 episodes - episode_reward: 21.500 [16.000, 26.000] - loss: 0.003 - mae: 1.497 - mean_q: 2.012 - mean_eps: 0.100 - ale.lives: 3.184\n",
            "\n",
            "Interval 134 (1330000 steps performed)\n",
            "10000/10000 [==============================] - 171s 17ms/step - reward: 0.0252\n",
            "11 episodes - episode_reward: 22.000 [12.000, 35.000] - loss: 0.003 - mae: 1.500 - mean_q: 2.016 - mean_eps: 0.100 - ale.lives: 3.293\n",
            "\n",
            "Interval 135 (1340000 steps performed)\n",
            "10000/10000 [==============================] - 173s 17ms/step - reward: 0.0247\n",
            "11 episodes - episode_reward: 23.273 [16.000, 30.000] - loss: 0.003 - mae: 1.507 - mean_q: 2.026 - mean_eps: 0.100 - ale.lives: 3.410\n",
            "\n",
            "Interval 136 (1350000 steps performed)\n",
            "10000/10000 [==============================] - 172s 17ms/step - reward: 0.0266\n",
            "11 episodes - episode_reward: 24.091 [15.000, 35.000] - loss: 0.003 - mae: 1.510 - mean_q: 2.030 - mean_eps: 0.100 - ale.lives: 3.276\n",
            "\n",
            "Interval 137 (1360000 steps performed)\n",
            "10000/10000 [==============================] - 173s 17ms/step - reward: 0.0258\n",
            "10 episodes - episode_reward: 25.200 [17.000, 34.000] - loss: 0.003 - mae: 1.516 - mean_q: 2.038 - mean_eps: 0.100 - ale.lives: 3.385\n",
            "\n",
            "Interval 138 (1370000 steps performed)\n",
            "10000/10000 [==============================] - 172s 17ms/step - reward: 0.0258\n",
            "10 episodes - episode_reward: 24.800 [14.000, 34.000] - loss: 0.003 - mae: 1.511 - mean_q: 2.031 - mean_eps: 0.100 - ale.lives: 3.483\n",
            "\n",
            "Interval 139 (1380000 steps performed)\n",
            "10000/10000 [==============================] - 171s 17ms/step - reward: 0.0259\n",
            "11 episodes - episode_reward: 24.455 [19.000, 33.000] - loss: 0.003 - mae: 1.511 - mean_q: 2.030 - mean_eps: 0.100 - ale.lives: 3.385\n",
            "\n",
            "Interval 140 (1390000 steps performed)\n",
            "10000/10000 [==============================] - 172s 17ms/step - reward: 0.0256\n",
            "11 episodes - episode_reward: 23.455 [19.000, 35.000] - loss: 0.003 - mae: 1.515 - mean_q: 2.035 - mean_eps: 0.100 - ale.lives: 3.296\n",
            "\n",
            "Interval 141 (1400000 steps performed)\n",
            "10000/10000 [==============================] - 172s 17ms/step - reward: 0.0235\n",
            "12 episodes - episode_reward: 20.000 [12.000, 29.000] - loss: 0.003 - mae: 1.521 - mean_q: 2.043 - mean_eps: 0.100 - ale.lives: 3.378\n",
            "\n",
            "Interval 142 (1410000 steps performed)\n",
            "10000/10000 [==============================] - 173s 17ms/step - reward: 0.0263\n",
            "9 episodes - episode_reward: 26.778 [11.000, 37.000] - loss: 0.003 - mae: 1.530 - mean_q: 2.054 - mean_eps: 0.100 - ale.lives: 3.164\n",
            "\n",
            "Interval 143 (1420000 steps performed)\n",
            "10000/10000 [==============================] - 173s 17ms/step - reward: 0.0254\n",
            "11 episodes - episode_reward: 23.000 [13.000, 34.000] - loss: 0.003 - mae: 1.532 - mean_q: 2.057 - mean_eps: 0.100 - ale.lives: 3.214\n",
            "\n",
            "Interval 144 (1430000 steps performed)\n",
            "10000/10000 [==============================] - 172s 17ms/step - reward: 0.0257\n",
            "12 episodes - episode_reward: 22.917 [16.000, 36.000] - loss: 0.003 - mae: 1.540 - mean_q: 2.067 - mean_eps: 0.100 - ale.lives: 3.244\n",
            "\n",
            "Interval 145 (1440000 steps performed)\n",
            "10000/10000 [==============================] - 173s 17ms/step - reward: 0.0250\n",
            "13 episodes - episode_reward: 18.692 [10.000, 25.000] - loss: 0.003 - mae: 1.544 - mean_q: 2.073 - mean_eps: 0.100 - ale.lives: 3.223\n",
            "\n",
            "Interval 146 (1450000 steps performed)\n",
            "10000/10000 [==============================] - 174s 17ms/step - reward: 0.0252\n",
            "11 episodes - episode_reward: 23.455 [11.000, 32.000] - loss: 0.003 - mae: 1.548 - mean_q: 2.077 - mean_eps: 0.100 - ale.lives: 3.206\n",
            "\n",
            "Interval 147 (1460000 steps performed)\n",
            "10000/10000 [==============================] - 173s 17ms/step - reward: 0.0264\n",
            "10 episodes - episode_reward: 24.800 [15.000, 35.000] - loss: 0.003 - mae: 1.566 - mean_q: 2.102 - mean_eps: 0.100 - ale.lives: 3.265\n",
            "\n",
            "Interval 148 (1470000 steps performed)\n",
            "10000/10000 [==============================] - 173s 17ms/step - reward: 0.0253\n",
            "11 episodes - episode_reward: 25.091 [20.000, 32.000] - loss: 0.003 - mae: 1.566 - mean_q: 2.103 - mean_eps: 0.100 - ale.lives: 3.252\n",
            "\n",
            "Interval 149 (1480000 steps performed)\n",
            "10000/10000 [==============================] - 174s 17ms/step - reward: 0.0253\n",
            "11 episodes - episode_reward: 21.727 [12.000, 33.000] - loss: 0.003 - mae: 1.568 - mean_q: 2.105 - mean_eps: 0.100 - ale.lives: 3.274\n",
            "\n",
            "Interval 150 (1490000 steps performed)\n",
            "10000/10000 [==============================] - 175s 17ms/step - reward: 0.0248\n",
            "12 episodes - episode_reward: 21.083 [11.000, 31.000] - loss: 0.003 - mae: 1.573 - mean_q: 2.112 - mean_eps: 0.100 - ale.lives: 3.271\n",
            "\n",
            "Interval 151 (1500000 steps performed)\n",
            "10000/10000 [==============================] - 174s 17ms/step - reward: 0.0263\n",
            "11 episodes - episode_reward: 23.909 [11.000, 30.000] - loss: 0.003 - mae: 1.576 - mean_q: 2.116 - mean_eps: 0.100 - ale.lives: 3.236\n",
            "\n",
            "Interval 152 (1510000 steps performed)\n",
            "10000/10000 [==============================] - 174s 17ms/step - reward: 0.0255\n",
            "11 episodes - episode_reward: 22.727 [14.000, 29.000] - loss: 0.003 - mae: 1.588 - mean_q: 2.131 - mean_eps: 0.100 - ale.lives: 3.121\n",
            "\n",
            "Interval 153 (1520000 steps performed)\n",
            "10000/10000 [==============================] - 175s 18ms/step - reward: 0.0258\n",
            "10 episodes - episode_reward: 24.800 [15.000, 34.000] - loss: 0.003 - mae: 1.588 - mean_q: 2.133 - mean_eps: 0.100 - ale.lives: 3.247\n",
            "\n",
            "Interval 154 (1530000 steps performed)\n",
            "10000/10000 [==============================] - 174s 17ms/step - reward: 0.0263\n",
            "11 episodes - episode_reward: 24.364 [17.000, 31.000] - loss: 0.003 - mae: 1.594 - mean_q: 2.142 - mean_eps: 0.100 - ale.lives: 3.166\n",
            "\n",
            "Interval 155 (1540000 steps performed)\n",
            "10000/10000 [==============================] - 176s 18ms/step - reward: 0.0266\n",
            "11 episodes - episode_reward: 24.636 [17.000, 35.000] - loss: 0.003 - mae: 1.607 - mean_q: 2.160 - mean_eps: 0.100 - ale.lives: 3.385\n",
            "\n",
            "Interval 156 (1550000 steps performed)\n",
            "10000/10000 [==============================] - 177s 18ms/step - reward: 0.0257\n",
            "9 episodes - episode_reward: 28.222 [21.000, 34.000] - loss: 0.003 - mae: 1.608 - mean_q: 2.160 - mean_eps: 0.100 - ale.lives: 3.187\n",
            "\n",
            "Interval 157 (1560000 steps performed)\n",
            "10000/10000 [==============================] - 175s 18ms/step - reward: 0.0254\n",
            "10 episodes - episode_reward: 25.600 [14.000, 36.000] - loss: 0.003 - mae: 1.604 - mean_q: 2.153 - mean_eps: 0.100 - ale.lives: 3.408\n",
            "\n",
            "Interval 158 (1570000 steps performed)\n",
            "10000/10000 [==============================] - 176s 18ms/step - reward: 0.0257\n",
            "10 episodes - episode_reward: 26.800 [20.000, 33.000] - loss: 0.003 - mae: 1.607 - mean_q: 2.157 - mean_eps: 0.100 - ale.lives: 3.358\n",
            "\n",
            "Interval 159 (1580000 steps performed)\n",
            "10000/10000 [==============================] - 175s 17ms/step - reward: 0.0253\n",
            "11 episodes - episode_reward: 21.727 [15.000, 29.000] - loss: 0.003 - mae: 1.610 - mean_q: 2.162 - mean_eps: 0.100 - ale.lives: 3.192\n",
            "\n",
            "Interval 160 (1590000 steps performed)\n",
            "10000/10000 [==============================] - 176s 18ms/step - reward: 0.0255\n",
            "10 episodes - episode_reward: 25.000 [19.000, 35.000] - loss: 0.003 - mae: 1.612 - mean_q: 2.163 - mean_eps: 0.100 - ale.lives: 3.433\n",
            "\n",
            "Interval 161 (1600000 steps performed)\n",
            "10000/10000 [==============================] - 176s 18ms/step - reward: 0.0263\n",
            "10 episodes - episode_reward: 26.100 [21.000, 39.000] - loss: 0.003 - mae: 1.612 - mean_q: 2.164 - mean_eps: 0.100 - ale.lives: 3.354\n",
            "\n",
            "Interval 162 (1610000 steps performed)\n",
            "10000/10000 [==============================] - 181s 18ms/step - reward: 0.0247\n",
            "12 episodes - episode_reward: 22.167 [13.000, 35.000] - loss: 0.003 - mae: 1.617 - mean_q: 2.169 - mean_eps: 0.100 - ale.lives: 3.092\n",
            "\n",
            "Interval 163 (1620000 steps performed)\n",
            "10000/10000 [==============================] - 177s 18ms/step - reward: 0.0260\n",
            "10 episodes - episode_reward: 25.200 [17.000, 39.000] - loss: 0.003 - mae: 1.614 - mean_q: 2.166 - mean_eps: 0.100 - ale.lives: 3.344\n",
            "\n",
            "Interval 164 (1630000 steps performed)\n",
            "10000/10000 [==============================] - 179s 18ms/step - reward: 0.0260\n",
            "10 episodes - episode_reward: 23.500 [13.000, 38.000] - loss: 0.003 - mae: 1.619 - mean_q: 2.172 - mean_eps: 0.100 - ale.lives: 3.399\n",
            "\n",
            "Interval 165 (1640000 steps performed)\n",
            "10000/10000 [==============================] - 177s 18ms/step - reward: 0.0254\n",
            "12 episodes - episode_reward: 23.917 [12.000, 47.000] - loss: 0.003 - mae: 1.621 - mean_q: 2.176 - mean_eps: 0.100 - ale.lives: 3.583\n",
            "\n",
            "Interval 166 (1650000 steps performed)\n",
            "10000/10000 [==============================] - 176s 18ms/step - reward: 0.0258\n",
            "10 episodes - episode_reward: 24.100 [14.000, 31.000] - loss: 0.003 - mae: 1.617 - mean_q: 2.170 - mean_eps: 0.100 - ale.lives: 3.382\n",
            "\n",
            "Interval 167 (1660000 steps performed)\n",
            "10000/10000 [==============================] - 175s 18ms/step - reward: 0.0252\n",
            "11 episodes - episode_reward: 24.818 [18.000, 36.000] - loss: 0.002 - mae: 1.617 - mean_q: 2.169 - mean_eps: 0.100 - ale.lives: 3.290\n",
            "\n",
            "Interval 168 (1670000 steps performed)\n",
            "10000/10000 [==============================] - 176s 18ms/step - reward: 0.0264\n",
            "10 episodes - episode_reward: 24.800 [15.000, 36.000] - loss: 0.002 - mae: 1.617 - mean_q: 2.169 - mean_eps: 0.100 - ale.lives: 3.172\n",
            "\n",
            "Interval 169 (1680000 steps performed)\n",
            "10000/10000 [==============================] - 174s 17ms/step - reward: 0.0255\n",
            "11 episodes - episode_reward: 24.273 [15.000, 37.000] - loss: 0.003 - mae: 1.614 - mean_q: 2.167 - mean_eps: 0.100 - ale.lives: 3.337\n",
            "\n",
            "Interval 170 (1690000 steps performed)\n",
            "10000/10000 [==============================] - 175s 17ms/step - reward: 0.0252\n",
            "10 episodes - episode_reward: 24.100 [17.000, 35.000] - loss: 0.003 - mae: 1.619 - mean_q: 2.173 - mean_eps: 0.100 - ale.lives: 3.317\n",
            "\n",
            "Interval 171 (1700000 steps performed)\n",
            "10000/10000 [==============================] - 174s 17ms/step - reward: 0.0259\n",
            "10 episodes - episode_reward: 24.900 [11.000, 35.000] - loss: 0.003 - mae: 1.621 - mean_q: 2.176 - mean_eps: 0.100 - ale.lives: 3.259\n",
            "\n",
            "Interval 172 (1710000 steps performed)\n",
            "10000/10000 [==============================] - 173s 17ms/step - reward: 0.0264\n",
            "11 episodes - episode_reward: 25.727 [15.000, 39.000] - loss: 0.003 - mae: 1.627 - mean_q: 2.184 - mean_eps: 0.100 - ale.lives: 3.131\n",
            "\n",
            "Interval 173 (1720000 steps performed)\n",
            "10000/10000 [==============================] - 177s 18ms/step - reward: 0.0262\n",
            "10 episodes - episode_reward: 24.700 [18.000, 33.000] - loss: 0.002 - mae: 1.627 - mean_q: 2.183 - mean_eps: 0.100 - ale.lives: 3.320\n",
            "\n",
            "Interval 174 (1730000 steps performed)\n",
            "10000/10000 [==============================] - 176s 18ms/step - reward: 0.0260\n",
            "10 episodes - episode_reward: 26.700 [20.000, 37.000] - loss: 0.003 - mae: 1.633 - mean_q: 2.191 - mean_eps: 0.100 - ale.lives: 3.322\n",
            "\n",
            "Interval 175 (1740000 steps performed)\n",
            "10000/10000 [==============================] - 173s 17ms/step - reward: 0.0264\n",
            "10 episodes - episode_reward: 26.100 [20.000, 31.000] - loss: 0.003 - mae: 1.635 - mean_q: 2.193 - mean_eps: 0.100 - ale.lives: 3.076\n",
            "\n",
            "Interval 176 (1750000 steps performed)\n",
            "10000/10000 [==============================] - 171s 17ms/step - reward: 0.0264\n",
            "10 episodes - episode_reward: 27.600 [18.000, 32.000] - loss: 0.003 - mae: 1.635 - mean_q: 2.193 - mean_eps: 0.100 - ale.lives: 3.300\n",
            "\n",
            "Interval 177 (1760000 steps performed)\n",
            "10000/10000 [==============================] - 170s 17ms/step - reward: 0.0253\n",
            "10 episodes - episode_reward: 24.600 [14.000, 34.000] - loss: 0.003 - mae: 1.632 - mean_q: 2.189 - mean_eps: 0.100 - ale.lives: 3.410\n",
            "\n",
            "Interval 178 (1770000 steps performed)\n",
            "10000/10000 [==============================] - 169s 17ms/step - reward: 0.0260\n",
            "10 episodes - episode_reward: 24.800 [11.000, 32.000] - loss: 0.002 - mae: 1.640 - mean_q: 2.201 - mean_eps: 0.100 - ale.lives: 3.453\n",
            "\n",
            "Interval 179 (1780000 steps performed)\n",
            "10000/10000 [==============================] - 170s 17ms/step - reward: 0.0266\n",
            "10 episodes - episode_reward: 27.100 [18.000, 36.000] - loss: 0.003 - mae: 1.634 - mean_q: 2.192 - mean_eps: 0.100 - ale.lives: 3.259\n",
            "\n",
            "Interval 180 (1790000 steps performed)\n",
            "10000/10000 [==============================] - 171s 17ms/step - reward: 0.0271\n",
            "9 episodes - episode_reward: 28.778 [23.000, 34.000] - loss: 0.003 - mae: 1.637 - mean_q: 2.197 - mean_eps: 0.100 - ale.lives: 3.425\n",
            "\n",
            "Interval 181 (1800000 steps performed)\n",
            "10000/10000 [==============================] - 171s 17ms/step - reward: 0.0267\n",
            "10 episodes - episode_reward: 28.300 [14.000, 41.000] - loss: 0.003 - mae: 1.639 - mean_q: 2.199 - mean_eps: 0.100 - ale.lives: 3.263\n",
            "\n",
            "Interval 182 (1810000 steps performed)\n",
            "10000/10000 [==============================] - 172s 17ms/step - reward: 0.0261\n",
            "11 episodes - episode_reward: 24.091 [19.000, 31.000] - loss: 0.003 - mae: 1.654 - mean_q: 2.219 - mean_eps: 0.100 - ale.lives: 3.319\n",
            "\n",
            "Interval 183 (1820000 steps performed)\n",
            "10000/10000 [==============================] - 170s 17ms/step - reward: 0.0263\n",
            "11 episodes - episode_reward: 24.909 [18.000, 32.000] - loss: 0.003 - mae: 1.655 - mean_q: 2.221 - mean_eps: 0.100 - ale.lives: 3.071\n",
            "\n",
            "Interval 184 (1830000 steps performed)\n",
            "10000/10000 [==============================] - 171s 17ms/step - reward: 0.0263\n",
            "10 episodes - episode_reward: 25.800 [17.000, 42.000] - loss: 0.003 - mae: 1.660 - mean_q: 2.227 - mean_eps: 0.100 - ale.lives: 3.326\n",
            "\n",
            "Interval 185 (1840000 steps performed)\n",
            "10000/10000 [==============================] - 171s 17ms/step - reward: 0.0259\n",
            "9 episodes - episode_reward: 28.778 [22.000, 35.000] - loss: 0.003 - mae: 1.666 - mean_q: 2.236 - mean_eps: 0.100 - ale.lives: 3.261\n",
            "\n",
            "Interval 186 (1850000 steps performed)\n",
            "10000/10000 [==============================] - 170s 17ms/step - reward: 0.0256\n",
            "10 episodes - episode_reward: 25.200 [16.000, 38.000] - loss: 0.003 - mae: 1.662 - mean_q: 2.231 - mean_eps: 0.100 - ale.lives: 3.206\n",
            "\n",
            "Interval 187 (1860000 steps performed)\n",
            "10000/10000 [==============================] - 170s 17ms/step - reward: 0.0265\n",
            "9 episodes - episode_reward: 27.556 [19.000, 37.000] - loss: 0.003 - mae: 1.667 - mean_q: 2.238 - mean_eps: 0.100 - ale.lives: 3.519\n",
            "\n",
            "Interval 188 (1870000 steps performed)\n",
            "10000/10000 [==============================] - 171s 17ms/step - reward: 0.0257\n",
            "11 episodes - episode_reward: 25.000 [13.000, 36.000] - loss: 0.003 - mae: 1.673 - mean_q: 2.246 - mean_eps: 0.100 - ale.lives: 3.276\n",
            "\n",
            "Interval 189 (1880000 steps performed)\n",
            "10000/10000 [==============================] - 171s 17ms/step - reward: 0.0258\n",
            "10 episodes - episode_reward: 24.700 [12.000, 33.000] - loss: 0.003 - mae: 1.682 - mean_q: 2.257 - mean_eps: 0.100 - ale.lives: 3.259\n",
            "\n",
            "Interval 190 (1890000 steps performed)\n",
            "10000/10000 [==============================] - 172s 17ms/step - reward: 0.0258\n",
            "9 episodes - episode_reward: 28.778 [19.000, 41.000] - loss: 0.003 - mae: 1.690 - mean_q: 2.269 - mean_eps: 0.100 - ale.lives: 3.210\n",
            "\n",
            "Interval 191 (1900000 steps performed)\n",
            "10000/10000 [==============================] - 171s 17ms/step - reward: 0.0259\n",
            "11 episodes - episode_reward: 24.364 [12.000, 38.000] - loss: 0.003 - mae: 1.698 - mean_q: 2.280 - mean_eps: 0.100 - ale.lives: 3.203\n",
            "\n",
            "Interval 192 (1910000 steps performed)\n",
            "10000/10000 [==============================] - 170s 17ms/step - reward: 0.0264\n",
            "10 episodes - episode_reward: 27.000 [15.000, 41.000] - loss: 0.003 - mae: 1.704 - mean_q: 2.288 - mean_eps: 0.100 - ale.lives: 3.384\n",
            "\n",
            "Interval 193 (1920000 steps performed)\n",
            "10000/10000 [==============================] - 171s 17ms/step - reward: 0.0259\n",
            "10 episodes - episode_reward: 26.100 [13.000, 33.000] - loss: 0.003 - mae: 1.704 - mean_q: 2.287 - mean_eps: 0.100 - ale.lives: 3.277\n",
            "\n",
            "Interval 194 (1930000 steps performed)\n",
            "10000/10000 [==============================] - 174s 17ms/step - reward: 0.0262\n",
            "9 episodes - episode_reward: 28.111 [16.000, 35.000] - loss: 0.003 - mae: 1.706 - mean_q: 2.289 - mean_eps: 0.100 - ale.lives: 3.405\n",
            "\n",
            "Interval 195 (1940000 steps performed)\n",
            "10000/10000 [==============================] - 174s 17ms/step - reward: 0.0258\n",
            "9 episodes - episode_reward: 26.667 [21.000, 32.000] - loss: 0.003 - mae: 1.714 - mean_q: 2.300 - mean_eps: 0.100 - ale.lives: 3.348\n",
            "\n",
            "Interval 196 (1950000 steps performed)\n",
            "10000/10000 [==============================] - 173s 17ms/step - reward: 0.0257\n",
            "10 episodes - episode_reward: 28.400 [22.000, 37.000] - loss: 0.003 - mae: 1.725 - mean_q: 2.315 - mean_eps: 0.100 - ale.lives: 3.196\n",
            "\n",
            "Interval 197 (1960000 steps performed)\n",
            " 8525/10000 [========================>.....] - ETA: 25s - reward: 0.0265"
          ]
        }
      ],
      "source": [
        "from __future__ import division\n",
        "import argparse\n",
        "\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import gym\n",
        "\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Flatten, Convolution2D, Permute\n",
        "from keras.optimizers import Adam\n",
        "import keras.backend as K\n",
        "\n",
        "import rl\n",
        "from rl.agents.dqn import DQNAgent\n",
        "from rl.policy import LinearAnnealedPolicy, BoltzmannQPolicy, EpsGreedyQPolicy\n",
        "from rl.memory import SequentialMemory\n",
        "from rl.core import Processor\n",
        "from rl.callbacks import FileLogger, ModelIntervalCheckpoint\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "\n",
        "mode = \"train\"\n",
        "\n",
        "WINDOW_LENGTH = 1\n",
        "\n",
        "\n",
        "class AtariProcessor(Processor):\n",
        "    def process_observation(self, observation):\n",
        "        assert observation.ndim == 3  # (height, width, channel)\n",
        "        return observation.astype('uint8')\n",
        "\n",
        "    def process_state_batch(self, batch):\n",
        "        # We could perform this processing step in `process_observation`. In this case, however,\n",
        "        # we would need to store a `float32` array instead, which is 4x more memory intensive than\n",
        "        # an `uint8` array. This matters if we store 1M observations.\n",
        "        processed_batch = batch.astype('float32') / 255.\n",
        "        return processed_batch.reshape((processed_batch.shape[0], 84, 84, 4))\n",
        "\n",
        "    def process_reward(self, reward):\n",
        "        return np.clip(reward, -1., 1.)\n",
        "\n",
        "\n",
        "\n",
        "# Get the environment and extract the number of actions.\n",
        "env = Atari(\"BreakoutNoFrameskip-v4\", 84, 84, frame_skip=4)\n",
        "np.random.seed(456)\n",
        "env.env.seed(456)\n",
        "nb_actions = env.action_space_size\n",
        "\n",
        "# first layer takes in the 4 grayscale cropped image\n",
        "input_lyr = keras.layers.Input((84, 84, 4), name=\"Input_last_4_frames\")\n",
        "\n",
        "# second layer convolves 16 8x8 then applies ReLU activation\n",
        "x = keras.layers.Conv2D(16, (8,8), strides=4, name=\"Hidden_layer_1\")(input_lyr)\n",
        "x = keras.layers.Activation('relu')(x)\n",
        "\n",
        "# third layer is the same but with 32 4x4 filters\n",
        "x = keras.layers.Conv2D(32, (4,4), strides=2, name=\"Hidden_layer_2\")(x)\n",
        "x = keras.layers.Activation('relu')(x)\n",
        "\n",
        "# James Note: Missing final dense hidden layer:\n",
        "x = keras.layers.Flatten(name=\"Flatten\")(x)\n",
        "x = keras.layers.Dense(256, name=\"Hidden_layer_3\")(x)\n",
        "x = keras.layers.Activation('relu')(x)\n",
        "\n",
        "# output layer is a fullyconnected linear layer\n",
        "x = keras.layers.Dense(nb_actions, activation='linear')(x)\n",
        "\n",
        "model = keras.Model(inputs=input_lyr, outputs=x, name=\"ATARI_DQN\")\n",
        "\n",
        "\n",
        "# Finally, we configure and compile our agent. You can use every built-in Keras optimizer and\n",
        "# even the metrics!\n",
        "memory = SequentialMemory(limit=250_000, window_length=WINDOW_LENGTH)\n",
        "processor = AtariProcessor()\n",
        "\n",
        "# Select a policy. We use eps-greedy action selection, which means that a random action is selected\n",
        "# with probability eps. We anneal eps from 1.0 to 0.1 over the course of 1M steps. This is done so that\n",
        "# the agent initially explores the environment (high eps) and then gradually sticks to what it knows\n",
        "# (low eps). We also set a dedicated eps value that is used during testing. Note that we set it to 0.05\n",
        "# so that the agent still performs some random actions. This ensures that the agent cannot get stuck.\n",
        "policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), attr='eps', value_max=1., value_min=.1, value_test=.05, nb_steps=1000000)\n",
        "\n",
        "# The trade-off between exploration and exploitation is difficult and an on-going research topic.\n",
        "# If you want, you can experiment with the parameters or use a different policy. Another popular one\n",
        "# is Boltzmann-style exploration:\n",
        "# policy = BoltzmannQPolicy(tau=1.)\n",
        "# Feel free to give it a try!\n",
        "\n",
        "dqn = DQNAgent(model=model, nb_actions=nb_actions, policy=policy, memory=memory,\n",
        "               processor=processor, nb_steps_warmup=100_000, gamma=.99, target_model_update=10000,\n",
        "               train_interval=4, delta_clip=1.)\n",
        "dqn.compile(Adam(lr=.00025), metrics=['mae'])\n",
        "\n",
        "if mode == 'train':\n",
        "    # Okay, now it's time to learn something! We capture the interrupt exception so that training\n",
        "    # can be prematurely aborted. Notice that now you can use the built-in Keras callbacks!\n",
        "    weights_filename = 'dqn_{}_weights_100k_250k_5m.h5f'.format(\"Breakout-v0\")\n",
        "    checkpoint_weights_filename = 'dqn_' + \"Breakout-v0\" + '_weights_{step}.h5f'\n",
        "    log_filename = 'dqn_{}_log.json'.format(\"Breakout-v0\")\n",
        "    callbacks = [ModelIntervalCheckpoint(checkpoint_weights_filename, interval=250000)]\n",
        "    callbacks += [FileLogger(log_filename, interval=5)]\n",
        "    dqn.fit(env, callbacks=callbacks, nb_steps=5_000_000, log_interval=10000)\n",
        "\n",
        "    # After training is done, we save the final weights one more time.\n",
        "    dqn.save_weights(weights_filename, overwrite=True)\n",
        "\n",
        "    # Finally, evaluate our algorithm for 10 episodes.\n",
        "    dqn.test(env, nb_episodes=10, visualize=False)\n",
        "elif mode == 'test':\n",
        "    weights_filename = 'dqn_{}_weights.h5f'.format(\"Breakout-v0\")\n",
        "    if args.weights:\n",
        "        weights_filename = args.weights\n",
        "    dqn.load_weights(weights_filename)\n",
        "    dqn.test(env, nb_episodes=10, visualize=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        },
        "id": "_xzNBmpLUXb7",
        "outputId": "3c6bf629-764a-42ed-f9e3-693b6fcee47b"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-fd5dede08c85>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ],
      "source": [
        "model.save(\"q_agent_model.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k5wL8kebBpQX"
      },
      "outputs": [],
      "source": [
        "\"\""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "KerasRLPaper.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
